{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py\n",
        "!pip install POT\n",
        "!nvidia-smi\n",
        "!pip install -q --upgrade cupy-cuda12x\n",
        "!pip install softimpute         # notice: no underscore\n",
        "# ============================================================================ #\n",
        "# CELL 1: Project Setup, Imports, Logging, Config\n",
        "# ============================================================================ #\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import re\n",
        "import gc\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict, Optional, Union, Callable, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "from scipy.sparse.linalg import svds, LinearOperator # Import LinearOperator\n",
        "from scipy.optimize import OptimizeResult # For line search return consistency\n",
        "from numpy.random import default_rng, Generator\n",
        "from sklearn.model_selection import train_test_split # For train/validation split\n",
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive # Uncomment if using Colab\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "DRIVE_MOUNTED = True\n",
        "# right after the imports\n",
        "import logging\n",
        "logging.disable(logging.WARNING)   # hides all warnings emitted via logging\n",
        "\n",
        "# === ADDED Block 5 (MPI) ===\n",
        "try:\n",
        "    from mpi4py import MPI\n",
        "    COMM = MPI.COMM_WORLD\n",
        "    RANK_MPI = COMM.Get_rank()\n",
        "    SIZE_MPI = COMM.Get_size()\n",
        "    if RANK_MPI == 0: print(f\"+++ MPI Detected: Running with {SIZE_MPI} processes. +++\")\n",
        "except ImportError:\n",
        "    COMM = None\n",
        "    RANK_MPI = 0\n",
        "    SIZE_MPI = 1\n",
        "    # print(\"+++ MPI Not Found: Running in serial mode. +++\") # Less verbose\n",
        "\n",
        "# === ADDED Block 6 === (Import for OT demo)\n",
        "try:\n",
        "    import ot\n",
        "    OT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OT_AVAILABLE = False\n",
        "    if RANK_MPI == 0: print(\"Warning: POT library not found. Skipping Barycentre demo.\")\n",
        "\n",
        "# === ADDED Block 6 (PCA) ===\n",
        "try:\n",
        "    from sklearn.decomposition import PCA\n",
        "    PCA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PCA_AVAILABLE = False\n",
        "    if RANK_MPI == 0: print(\"Warning: sklearn not found. Skipping PCA trajectory plot.\")\n",
        "\n",
        "\n",
        "# --- Logging Setup (Initialize Logger FIRST) ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    force=True, # Overwrite any existing config\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Mount Drive ---\n",
        "if RANK_MPI == 0: print(\"+++ Mounting Google Drive +++\")\n",
        "try:\n",
        "    # Only rank 0 should try to force remount if needed\n",
        "    drive.mount('/content/drive', force_remount=(RANK_MPI == 0))\n",
        "    if RANK_MPI == 0: print(\"Drive mounted.\")\n",
        "    if COMM and SIZE_MPI > 1: COMM.Barrier() # Ensure drive is mounted\n",
        "except Exception as e:\n",
        "    if RANK_MPI == 0: print(f\"Error mounting drive: {e}\")\n",
        "    if COMM and SIZE_MPI > 1: COMM.Abort()\n",
        "    raise\n",
        "\n",
        "# --- Optional: Try importing CuPy for GPU acceleration ---\n",
        "# NOTE: Efficient SoftImpute implementation below uses SciPy sparse ops,\n",
        "# GPU acceleration would require re-implementing the LinearOperator with CuPy sparse.\n",
        "try:\n",
        "    import cupy as cp\n",
        "    import cupyx.scipy.sparse as cpx\n",
        "    CUPY_AVAILABLE = False # Disable GPU for SoftImpute for now due to LinearOperator complexity\n",
        "    logger.warning(\"CuPy found, but GPU acceleration for efficient SoftImpute is NOT enabled in this version.\")\n",
        "    if 'cp' not in locals(): cp = np\n",
        "    if 'cpx' not in locals(): cpx = sparse\n",
        "except ImportError:\n",
        "    CUPY_AVAILABLE = False\n",
        "    cp = np ; cpx = sparse\n",
        "    logger.warning(\"CuPy not found, will run on CPU using NumPy/SciPy.\")\n",
        "\n",
        "logger.info(\"+++ Cell 1: Setup, Imports, Logging, Config +++\")\n",
        "\n",
        "# --- Global Config ---\n",
        "# --- MOVIELENS 1M Configuration ---\n",
        "DATA_DIR_STR = \"/content/drive/MyDrive/ml-1m\" # ADJUST PATH AS NEEDED\n",
        "RATINGS_FILENAME = \"ratings.dat\"\n",
        "VALIDATION_FRACTION = 0.2 # Hold out 20% for validation\n",
        "# --- USE COMPLETE DATASET (FIX 1) ---\n",
        "RATING_LIMIT = None # Load all ratings from ml-1m\n",
        "RANK = 10 # Factorization rank (r in paper) for non-convex\n",
        "LAM = 1e-2 # Regularization parameter λ\n",
        "LAM_SQ = LAM ** 2 # λ^2 for non-convex model factor regularization\n",
        "LAM_BIAS = 1e-4 # Regularization for bias terms\n",
        "SEED = 0 # Use consistent seed from long.txt\n",
        "# --- INCREASED ITERATIONS ---\n",
        "N_ITERS_ALL = 20 # Iterations/epochs for ALL solvers\n",
        "CONVEX_RANK_K = 50 # Max rank for Soft-Impute intermediate SVDs\n",
        "SOFT_IMPUTE_TOL = 1e-4 # Convergence tolerance for Soft-Impute\n",
        "N_ITERS_CONVEX = N_ITERS_ALL # Use same number of iterations for SoftImpute\n",
        "# --- SVRG Params ---\n",
        "INIT_LR_SVRG = 1e-3 # Base Learning rate for SVRG inner solver\n",
        "SVRG_INNER_STEPS_DIVISOR = 1 # Use full inner pass\n",
        "GRAD_CLIP_THRESHOLD = 10.0 # Max norm for SVRG gradients before update\n",
        "RSVRG_BATCH_SIZE = 100 # Batch size for non-convex SVRG refresh step\n",
        "# --- ALS Params ---\n",
        "ALS_TOL = 1e-4 # Convergence tolerance for ALS based on RMSE change\n",
        "ALS_MAX_ITER = N_ITERS_ALL # Use same iter count as others for comparison\n",
        "# --- RGD/Accelerated Params ---\n",
        "INIT_LR_RIEMANN = 0.5 # Initial LR for RGD/RAGD/Catalyst/DANE line search\n",
        "LS_BETA = 0.5         # Line search reduction factor\n",
        "LS_SIGMA = 1e-4       # Sufficient decrease parameter\n",
        "RAGD_GAMMA = 1.0; RAGD_MU = 5.0; RAGD_BETA = 5.0\n",
        "DANE_KAPPA = 1.0\n",
        "KAPPA_0 = 1e-1; KAPPA_CVX = 1e-1; INNER_T = 5; INNER_S_BASE = 10; MAX_KAPPA_DOUBLINGS = 10\n",
        "# --- Smaller Initialization Scale ---\n",
        "INIT_SCALE_NON_CONVEX = 0.01 # Smaller scale for initial U, W\n",
        "# --- Configuration from Proposal/long.txt ---\n",
        "RETRACTION_NAME = \"orthonormal\"  # Options: \"orthonormal\", \"cayley\", \"projection\"\n",
        "REG_DISTANCE = \"euclid\"      # Options: \"euclid\", \"retraction\"\n",
        "INNER_SOLVER = \"svrg\"        # Options: \"svrg\", \"sarah\", \"spider\" (for Catalyst)\n",
        "ETA_GRAD = 1e-3              # Adaptive stopping tolerance for inner grad norm\n",
        "ETA_DIST = 1e-4              # Adaptive stopping tolerance for inner step size\n",
        "CATALYST_INNER_T_EPOCHS = 1 # Epochs for Alg phi_1 check budget\n",
        "CATALYST_INNER_S_EPOCHS_BASE = 2 # Base epochs for S_k schedule\n",
        "RSVRG_LR = 1e-3              # Step size for RSVRG/SARAH/SPIDER inner loops\n",
        "\n",
        "# --- Derived Globals ---\n",
        "GLOBAL_RNG = default_rng(SEED)\n",
        "DATA_DIR = Path(DATA_DIR_STR)\n",
        "I_r = np.eye(RANK, dtype=np.float64) # Identity matrix of size RANK\n",
        "\n",
        "# Check Data Directory\n",
        "if DRIVE_MOUNTED and not DATA_DIR.is_dir():\n",
        "    if RANK_MPI == 0: logger.warning(f\"DATA_DIR '{DATA_DIR}' not found. Please check the path.\")\n",
        "elif not DRIVE_MOUNTED:\n",
        "     if RANK_MPI == 0: logger.warning(f\"Google Drive not mounted.\")\n",
        "\n",
        "logger.info(\"Cell 1 initialisation complete.\")\n",
        "\n",
        "# ============================================================================ #\n",
        "# CELL 2: Data Loading and Preprocessing (MovieLens 1M)\n",
        "# ============================================================================ #\n",
        "logger.info(\"+++ Cell 2: Loading and Processing Data (MovieLens 1M) +++\")\n",
        "# --- Manifold Operations ---\n",
        "# --- universal 2-tuple helper for loss/grad (used by Catalyst) ---\n",
        "def stochastic_gradient_batch(U, user_ids, N_users, N_movies, loss_args):\n",
        "    \"\"\"\n",
        "    Vectorised version of `stochastic_gradient_single_user`.\n",
        "    Accumulates the (un-scaled) gradient over the provided user_ids.\n",
        "    \"\"\"\n",
        "    G = np.zeros_like(U, dtype=np.float32)\n",
        "    for uid in user_ids:\n",
        "        G += stochastic_gradient_single_user(U, int(uid), N_users, N_movies, loss_args)\n",
        "    return G / max(1, len(user_ids))          # average over the batch\n",
        "\n",
        "def loss_and_grad_corrected(U, W, bu, bi, *rest):\n",
        "    \"\"\" Wrapper for loss_and_grad_serial_with_biases to return only loss and grad_U. \"\"\"\n",
        "    # Calls the main loss function which handles MPI reduction\n",
        "    loss, gU, *_ = loss_and_grad_serial_with_biases(U, W, bu, bi, *rest)\n",
        "    return loss, gU # 2-tuple exactly as Catalyst expects\n",
        "def OrthRetraction(U: np.ndarray, V: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    QR-based retraction to the Stiefel / Grassmann manifold.\n",
        "    Uses *reduced* QR so it works on NumPy ≥1.26 and CuPy.\n",
        "    \"\"\"\n",
        "    # Handle potential zero V vector to avoid QR issues\n",
        "    if np.linalg.norm(V) < 1e-12:\n",
        "        return U.astype(np.float32)\n",
        "\n",
        "    # --- FIX: Check for non-finite input ---\n",
        "    UV = U + V\n",
        "    if not np.isfinite(UV).all():\n",
        "        logger.warning(\"OrthRetraction: Input U+V contains non-finite values. Returning original U.\")\n",
        "        return U.astype(np.float32)\n",
        "    # --------------------------------------\n",
        "\n",
        "    try:\n",
        "        # --- FIX: Use mode='reduced' ---\n",
        "        Q, R_qr = np.linalg.qr(UV, mode='reduced')\n",
        "        # ------------------------------\n",
        "\n",
        "        # Ensure Q has the same shape as U\n",
        "        if Q.shape[1] < U.shape[1]:\n",
        "             pad_width = U.shape[1] - Q.shape[1]\n",
        "             Q = np.pad(Q, ((0, 0), (0, pad_width)), mode='constant')\n",
        "             logger.warning(f\"OrthRetraction: Padded Q due to rank collapse (V norm: {np.linalg.norm(V):.2e})\")\n",
        "        # Optional: Fix sign ambiguity by matching diagonal of R_qr to be positive\n",
        "        # sign_diag = np.sign(np.diag(R_qr))\n",
        "        # sign_diag[sign_diag == 0] = 1 # Avoid multiplying by zero\n",
        "        # Q = Q @ np.diag(sign_diag)\n",
        "        return Q.astype(np.float32)\n",
        "    except np.linalg.LinAlgError:\n",
        "        logger.warning(f\"OrthRetraction: QR decomposition failed (V norm: {np.linalg.norm(V):.2e}). Returning original U.\")\n",
        "        return U.astype(np.float32)\n",
        "    except ValueError as e: # Catch potential value errors from qr\n",
        "        logger.error(f\"OrthRetraction: ValueError during QR: {e}. Returning original U.\")\n",
        "        return U.astype(np.float32)\n",
        "    except Exception as e: # Catch any other unexpected errors\n",
        "        logger.error(f\"OrthRetraction failed with unexpected error: {e}\")\n",
        "        return U.astype(np.float32)\n",
        "# Initialize default values\n",
        "N_users_active, M_movies_active = 0, 0\n",
        "R_train_coo = sparse.coo_matrix((0, 0), dtype=np.float64)\n",
        "R_train_coo_orig = sparse.coo_matrix((0, 0), dtype=np.float64) # For original ratings\n",
        "R_train_csr_orig = sparse.csr_matrix((0,0), dtype=np.float64) # For SoftImpute _matvec\n",
        "R_train_csc_orig = sparse.csc_matrix((0,0), dtype=np.float64) # For SoftImpute _rmatvec\n",
        "ratings_train_orig = np.array([], dtype=np.float64) # Keep original ratings for viz\n",
        "ratings_train_centered = np.array([], dtype=np.float64)\n",
        "mapped_user_ids_train, mapped_movie_ids_train = np.array([], dtype=np.int32), np.array([], dtype=np.int32)\n",
        "user_ids_val_final, movie_ids_val_final, ratings_val_true = (np.array([], dtype=np.int32), np.array([], dtype=np.int32), np.array([], dtype=np.float64))\n",
        "global_mean_rating = 0.0\n",
        "user_map_global_to_local = {}\n",
        "movie_map_global_to_local = {}\n",
        "unique_users_train = np.array([], dtype=np.int32)\n",
        "unique_movies_train = np.array([], dtype=np.int32)\n",
        "DATA_AVAILABLE = False\n",
        "user_data_arrays = {} # Precompute user data for ALS/SVRG\n",
        "sampling_prob = None # Initialize sampling probability\n",
        "RSVRG_EPOCH_LEN = 1 # Default epoch length\n",
        "\n",
        "ratings_file_path = DATA_DIR / RATINGS_FILENAME\n",
        "\n",
        "if DRIVE_MOUNTED and ratings_file_path.is_file():\n",
        "    logger.info(f\"Loading MovieLens 1M data from: {ratings_file_path}\")\n",
        "    try:\n",
        "        ratings_df = pd.read_csv(\n",
        "            ratings_file_path, sep='::', header=None,\n",
        "            names=['user_id', 'movie_id', 'rating', 'timestamp'],\n",
        "            engine='python', encoding='latin-1'\n",
        "        )\n",
        "        logger.info(f\"Loaded {len(ratings_df)} ratings.\")\n",
        "        DATA_AVAILABLE = True\n",
        "\n",
        "        if RATING_LIMIT is not None and RATING_LIMIT > 0 and len(ratings_df) > RATING_LIMIT:\n",
        "             logger.info(f\"Subsampling ratings from {len(ratings_df)} to {RATING_LIMIT}\")\n",
        "             ratings_df = ratings_df.sample(n=RATING_LIMIT, random_state=SEED)\n",
        "\n",
        "        stratify_arg = ratings_df['user_id'] if RATING_LIMIT is None else None\n",
        "        if stratify_arg is None and RATING_LIMIT is not None:\n",
        "            logger.warning(\"Stratify is disabled due to RATING_LIMIT being set.\")\n",
        "        train_df, val_df = train_test_split(\n",
        "            ratings_df, test_size=VALIDATION_FRACTION, random_state=SEED, stratify=stratify_arg)\n",
        "        logger.info(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n",
        "\n",
        "        user_ids_train_orig = train_df['user_id'].values; movie_ids_train_orig = train_df['movie_id'].values\n",
        "        ratings_train_orig = train_df['rating'].values.astype(np.float64)\n",
        "        user_ids_val_orig = val_df['user_id'].values; movie_ids_val_orig = val_df['movie_id'].values\n",
        "        ratings_val_true = val_df['rating'].values.astype(np.float64) # Keep original for validation\n",
        "\n",
        "        global_mean_rating = ratings_train_orig.mean()\n",
        "        logger.info(f\"Global mean rating (training): {global_mean_rating:.4f}\")\n",
        "\n",
        "        unique_users_train, mapped_user_ids_train = np.unique(user_ids_train_orig, return_inverse=True)\n",
        "        unique_movies_train, mapped_movie_ids_train = np.unique(movie_ids_train_orig, return_inverse=True)\n",
        "        N_users_active = len(unique_users_train); M_movies_active = len(unique_movies_train)\n",
        "        user_map_global_to_local = {orig_id: local_id for local_id, orig_id in enumerate(unique_users_train)}\n",
        "        movie_map_global_to_local = {orig_id: local_id for local_id, orig_id in enumerate(unique_movies_train)}\n",
        "        logger.info(f\"Active users in training: {N_users_active}, Active movies in training: {M_movies_active}\")\n",
        "\n",
        "        ratings_train_centered = ratings_train_orig - global_mean_rating\n",
        "\n",
        "        val_user_mask = np.isin(user_ids_val_orig, unique_users_train)\n",
        "        val_movie_mask = np.isin(movie_ids_val_orig, unique_movies_train)\n",
        "        val_valid_mask = val_user_mask & val_movie_mask\n",
        "        user_ids_val_filt = user_ids_val_orig[val_valid_mask]; movie_ids_val_filt = movie_ids_val_orig[val_valid_mask]\n",
        "        ratings_val_true = ratings_val_true[val_valid_mask] # Filter true ratings accordingly\n",
        "        user_ids_val_final = np.array([user_map_global_to_local.get(uid, -1) for uid in user_ids_val_filt], dtype=np.int32)\n",
        "        movie_ids_val_final = np.array([movie_map_global_to_local.get(mid, -1) for mid in movie_ids_val_filt], dtype=np.int32)\n",
        "        valid_map_mask = (user_ids_val_final != -1) & (movie_ids_val_final != -1) # Filter out any potential misses\n",
        "        user_ids_val_final = user_ids_val_final[valid_map_mask]; movie_ids_val_final = movie_ids_val_final[valid_map_mask]\n",
        "        ratings_val_true = ratings_val_true[valid_map_mask] # Filter again after mapping\n",
        "        logger.info(f\"Validation pairs mapped to training users/movies: {len(user_ids_val_final)}\")\n",
        "\n",
        "        if ratings_train_centered.size > 0:\n",
        "            R_train_coo = sparse.coo_matrix((ratings_train_centered, (mapped_movie_ids_train, mapped_user_ids_train)), shape=(M_movies_active, N_users_active), dtype=np.float64)\n",
        "            R_train_coo.eliminate_zeros()\n",
        "            logger.info(f\"Built sparse training matrix (Centered) R_train_coo: shape={R_train_coo.shape}, nnz={R_train_coo.nnz}\")\n",
        "            R_train_coo_orig = sparse.coo_matrix((ratings_train_orig, (mapped_movie_ids_train, mapped_user_ids_train)), shape=(M_movies_active, N_users_active), dtype=np.float64)\n",
        "            R_train_coo_orig.eliminate_zeros()\n",
        "            R_train_csr_orig = R_train_coo_orig.tocsr(); R_train_csc_orig = R_train_coo_orig.tocsc()\n",
        "            logger.info(f\"Built sparse training matrix (Original) R_train_coo_orig: shape={R_train_coo_orig.shape}, nnz={R_train_coo_orig.nnz}\")\n",
        "\n",
        "            # Precompute user data structures for ALS/SVRG\n",
        "            logger.info(\"Precomputing user data structures...\")\n",
        "            t_precomp_start = time.time()\n",
        "            user_data_arrays = {}\n",
        "            for r, c, v in zip(R_train_coo_orig.row, R_train_coo_orig.col, R_train_coo_orig.data):\n",
        "                user_data_arrays.setdefault(c, []).append((r, v))\n",
        "            for u, rating_list in user_data_arrays.items():\n",
        "                if rating_list:\n",
        "                    movie_indices_list, rs_list = zip(*rating_list)\n",
        "                    user_data_arrays[u] = {'movies': np.array(list(movie_indices_list),dtype=np.int32),\n",
        "                                           'rs': np.array(list(rs_list),dtype=np.float64)} # Store original ratings\n",
        "            logger.info(f\"User data precomputation done in {time.time() - t_precomp_start:.2f}s\")\n",
        "            # Calculate importance sampling weights (consistent across ranks)\n",
        "            all_user_indices_global = np.array(list(user_data_arrays.keys()), dtype=np.int32)\n",
        "            num_active_users_global = len(all_user_indices_global)\n",
        "            user_weights = None; use_importance_sampling = False\n",
        "            if num_active_users_global > 0:\n",
        "                if RANK_MPI == 0: print(\"Calculating importance sampling weights...\")\n",
        "                user_ratings_count = [len(user_data_arrays[u_idx]['movies']) if u_idx in user_data_arrays and 'movies' in user_data_arrays[u_idx] else 0 for u_idx in all_user_indices_global]\n",
        "                user_weights_np = np.array(user_ratings_count, dtype=np.float64)\n",
        "                sum_weights = user_weights_np.sum()\n",
        "                if sum_weights > 1e-9:\n",
        "                    user_weights_np /= sum_weights\n",
        "                    user_weights = user_weights_np # Probabilities aligned with all_user_indices_global\n",
        "                    use_importance_sampling = True\n",
        "                    if RANK_MPI == 0: print(f\"Importance sampling enabled (weights based on {sum_weights:.0f} ratings).\")\n",
        "                else:\n",
        "                     if RANK_MPI == 0: print(\"Warning: Cannot compute importance sampling weights. Using uniform.\")\n",
        "            else:\n",
        "                 if RANK_MPI == 0: print(\"No active users, cannot use importance sampling.\")\n",
        "            sampling_prob = user_weights if use_importance_sampling else None\n",
        "            RSVRG_EPOCH_LEN = math.ceil(num_active_users_global / RSVRG_BATCH_SIZE) if num_active_users_global > 0 else 1\n",
        "            if RANK_MPI == 0: print(f\"RSVRG Epoch Length set to {RSVRG_EPOCH_LEN} batches.\")\n",
        "\n",
        "        else: logger.error(\"No training ratings available.\")\n",
        "\n",
        "    except FileNotFoundError: logger.error(f\"MovieLens file not found: {ratings_file_path}\"); DATA_AVAILABLE = False\n",
        "    except Exception as e: logger.error(f\"Error processing MovieLens: {e}\", exc_info=True); DATA_AVAILABLE = False\n",
        "elif not DRIVE_MOUNTED: logger.error(\"Google Drive not mounted.\")\n",
        "else: logger.error(f\"Data directory {DATA_DIR} or ratings file {RATINGS_FILENAME} not found.\")\n",
        "\n",
        "gc.collect()\n",
        "logger.info(\"Cell 2: Data Loading and Preprocessing Complete.\")\n",
        "logger.info(f\"Active Dimensions: M_movies={M_movies_active}, N_users={N_users_active}\")\n",
        "logger.info(f\"Training Ratings: {R_train_coo.nnz}\")\n",
        "logger.info(f\"Validation Ratings (for RMSE): {ratings_val_true.size}\")\n",
        "\n",
        "\n",
        "# ============================================================================ #\n",
        "# CELL 2.5: Data Visualization\n",
        "# ============================================================================ #\n",
        "logger.info(\"+++ Cell 2.5: Visualizing Loaded Data +++\")\n",
        "\n",
        "if RANK_MPI == 0: # Only rank 0 should plot\n",
        "    if DATA_AVAILABLE and ratings_train_orig.size > 0:\n",
        "        plt.style.use('seaborn-v0_8-whitegrid') # Use a nice style\n",
        "\n",
        "        # 1. Rating Distribution\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        counts, bins, patches = plt.hist(ratings_train_orig, bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], rwidth=0.8, align='mid', color='skyblue', edgecolor='black')\n",
        "        bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
        "        for count, x in zip(counts, bin_centers):\n",
        "            if count > 0: plt.text(x, count, str(int(count)), ha='center', va='bottom')\n",
        "        plt.title('Distribution of Training Ratings (MovieLens 1M Subset)')\n",
        "        plt.xlabel('Rating'); plt.ylabel('Frequency')\n",
        "        plt.xticks([1, 2, 3, 4, 5]); plt.grid(axis='y', alpha=0.75)\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        # 2. Ratings per User\n",
        "        user_rating_counts = np.bincount(mapped_user_ids_train)\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.hist(user_rating_counts[user_rating_counts > 0], bins=50, log=True, color='lightcoral', edgecolor='black')\n",
        "        plt.title('Distribution of Ratings per User (Training Set)')\n",
        "        plt.xlabel('Number of Ratings Given'); plt.ylabel('Number of Users (log scale)')\n",
        "        plt.grid(axis='y', alpha=0.75); plt.tight_layout(); plt.show()\n",
        "\n",
        "        # 3. Ratings per Movie\n",
        "        movie_rating_counts = np.bincount(mapped_movie_ids_train)\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.hist(movie_rating_counts[movie_rating_counts > 0], bins=50, log=True, color='lightgreen', edgecolor='black')\n",
        "        plt.title('Distribution of Ratings per Movie (Training Set)')\n",
        "        plt.xlabel('Number of Ratings Received'); plt.ylabel('Number of Movies (log scale)')\n",
        "        plt.grid(axis='y', alpha=0.75); plt.tight_layout(); plt.show()\n",
        "        logger.info(\"Cell 2.5: Data Visualization Complete.\")\n",
        "    else:\n",
        "        logger.warning(\"Skipping data visualization as no data was loaded.\")\n",
        "\n",
        "# ============================================================================ #\n",
        "# CELL 3: Model Helpers (CONSOLIDATED)\n",
        "# ============================================================================ #\n",
        "logger.info(\"+++ Cell 3: Defining ALL Model Helpers +++\")\n",
        "\n",
        "# --- Retraction Factory ---\n",
        "class RetractionFactory:\n",
        "    _registry = {}\n",
        "    @classmethod\n",
        "    def register(cls, name):\n",
        "        def decorator(fn): cls._registry[name] = fn; return fn\n",
        "        return decorator\n",
        "    @classmethod\n",
        "    def get(cls, name):\n",
        "        if name not in cls._registry: raise KeyError(f\"Unknown retraction '{name}'. Available: {list(cls._registry.keys())}\")\n",
        "        return cls._registry[name]\n",
        "# --- Register Retractions ---\n",
        "@RetractionFactory.register(\"orthonormal\")\n",
        "def _retract_qr(U: np.ndarray, V: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"QR-based retraction.\"\"\"\n",
        "    if np.linalg.norm(V) < 1e-12: return U.astype(np.float32)\n",
        "    UV = U + V\n",
        "    if not np.isfinite(UV).all(): logger.warning(\"OrthRetraction: Input U+V non-finite.\"); return U.astype(np.float32)\n",
        "    try:\n",
        "        Q, R_qr = np.linalg.qr(UV, mode='reduced') # Use 'reduced'\n",
        "        if Q.shape[1] < U.shape[1]:\n",
        "             pad_width = U.shape[1] - Q.shape[1]; Q = np.pad(Q, ((0, 0), (0, pad_width)), mode='constant')\n",
        "             logger.warning(f\"OrthRetraction: Padded Q\")\n",
        "        return Q.astype(np.float32)\n",
        "    except Exception as e: logger.error(f\"OrthRetraction failed: {e}\"); return U.astype(np.float32)\n",
        "@RetractionFactory.register(\"cayley\")\n",
        "def _retract_cayley(U: np.ndarray, V: np.ndarray, alpha: float = 0.1) -> np.ndarray:\n",
        "    \"\"\" Simple Cayley approx using QR of ambient step. \"\"\"\n",
        "    return _retract_qr(U, alpha * V)\n",
        "@RetractionFactory.register(\"projection\")\n",
        "def _retract_projection(U: np.ndarray, V: np.ndarray) -> np.ndarray:\n",
        "    \"\"\" Projection (polar decomposition) retraction. \"\"\"\n",
        "    U64 = U.astype(np.float64, copy=False); V64 = V.astype(np.float64, copy=False)\n",
        "    Z = U64 + V64; G = Z.T @ Z\n",
        "    try:\n",
        "        s, P = np.linalg.eigh(G); s_safe = np.maximum(s, 1e-12)\n",
        "        s_inv_sqrt = 1.0 / np.sqrt(s_safe); G_mhalf = P @ np.diag(s_inv_sqrt) @ P.T\n",
        "        result = (Z @ G_mhalf).astype(np.float32)\n",
        "        if result.shape != U.shape: logger.warning(f\"Projection Retraction Warning: Shape mismatch. Falling back to QR.\"); return _retract_qr(U, V)\n",
        "        return result\n",
        "    except Exception as e: logger.warning(f\"Projection Retraction Warning: {e}. Falling back to QR.\"); return _retract_qr(U, V)\n",
        "# --- Get the chosen retraction function ---\n",
        "R_fn = RetractionFactory.get(RETRACTION_NAME)\n",
        "if RANK_MPI == 0: logger.info(f\"Using Retraction: {RETRACTION_NAME}\")\n",
        "\n",
        "# --- Other Manifold Helpers ---\n",
        "def ProjTangent(U: np.ndarray, G: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Project G onto tangent space at U (Grassmann).\"\"\"\n",
        "    return (G - U @ (U.T @ G)).astype(np.float32)\n",
        "def LogMapApprox(U_base: np.ndarray, U_target: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Approximate inverse retraction (log map).\"\"\"\n",
        "    return ProjTangent(U_base, U_target - U_base)\n",
        "def RegularizeGradChordalApprox(U: np.ndarray, U_old: np.ndarray, kappa: float) -> np.ndarray:\n",
        "    \"\"\"Approximate gradient of distance regularization term.\"\"\"\n",
        "    U = U.astype(np.float32); U_old = U_old.astype(np.float32);\n",
        "    if REG_DISTANCE == \"euclid\": S = U.T @ U_old; grad_ambient = U @ (S - S.T); return kappa * ProjTangent(U, grad_ambient)\n",
        "    elif REG_DISTANCE == \"retraction\": v = LogMapApprox(U, U_old); return -kappa * v\n",
        "    else: raise ValueError(f\"Unknown REG_DISTANCE type: {REG_DISTANCE}\")\n",
        "\n",
        "# --- RMSE Evaluation ---\n",
        "def evaluate_rmse_with_biases(\n",
        "    U: np.ndarray, W: np.ndarray,\n",
        "    user_bias: np.ndarray, movie_bias: np.ndarray, global_mean: float,\n",
        "    probe_users_mapped: np.ndarray, probe_movies_mapped: np.ndarray, probe_ratings_true: np.ndarray # Now contains true ratings\n",
        ") -> float:\n",
        "    \"\"\"Computes RMSE on the validation set including bias terms and clamping.\"\"\"\n",
        "    if probe_ratings_true.size == 0: return np.nan # Check if validation set is empty\n",
        "    U = U.astype(np.float64, copy=False); W = W.astype(np.float64, copy=False)\n",
        "    user_bias = user_bias.astype(np.float64, copy=False); movie_bias = movie_bias.astype(np.float64, copy=False)\n",
        "    local_sum_sq_err = 0.0; local_count = 0\n",
        "    try:\n",
        "        if M_movies_active == 0 or N_users_active == 0: return np.nan\n",
        "        if probe_movies_mapped.size > 0 and (probe_movies_mapped.max() >= M_movies_active or probe_movies_mapped.min() < 0): return np.nan\n",
        "        if probe_users_mapped.size > 0 and (probe_users_mapped.max() >= N_users_active or probe_users_mapped.min() < 0): return np.nan\n",
        "        dot_prods = np.array([np.dot(U[m, :], W[:, u]) for m, u in zip(probe_movies_mapped, probe_users_mapped)], dtype=np.float64)\n",
        "        preds_raw = global_mean + user_bias[probe_users_mapped] + movie_bias[probe_movies_mapped] + dot_prods\n",
        "        preds_clamped = np.clip(preds_raw, 1.0, 5.0)\n",
        "        if not np.isfinite(preds_clamped).all(): preds_clamped = np.nan_to_num(preds_clamped, nan=global_mean)\n",
        "        if not np.isfinite(probe_ratings_true).all(): probe_ratings_true = np.nan_to_num(probe_ratings_true)\n",
        "        squared_errors = (preds_clamped - probe_ratings_true)**2\n",
        "        local_sum_sq_err = np.sum(squared_errors)\n",
        "        local_count = len(squared_errors)\n",
        "    except IndexError as e: logger.error(f\"IndexError during biased RMSE: {e}\"); return np.nan\n",
        "    except Exception as e: logger.error(f\"Error during biased RMSE: {e}\"); return np.nan\n",
        "    # --- MPI Reduction for RMSE ---\n",
        "    if COMM and SIZE_MPI > 1:\n",
        "        global_sum_sq_err_buf = np.array(local_sum_sq_err, dtype=np.float64); global_count_buf = np.array(local_count, dtype=np.int64)\n",
        "        global_sum_sq_err = np.array(0.0, dtype=np.float64); global_count = np.array(0, dtype=np.int64)\n",
        "        COMM.Allreduce(global_sum_sq_err_buf, global_sum_sq_err, op=MPI.SUM); COMM.Allreduce(global_count_buf, global_count, op=MPI.SUM)\n",
        "        if global_count > 0: mean_squared_error = global_sum_sq_err / global_count\n",
        "        else: return np.nan\n",
        "    else: # Serial case\n",
        "        if local_count > 0: mean_squared_error = local_sum_sq_err / local_count\n",
        "        else: return np.nan\n",
        "    mean_squared_error = max(0.0, mean_squared_error); rmse = np.sqrt(mean_squared_error)\n",
        "    return float(rmse) if np.isfinite(rmse) else np.nan\n",
        "\n",
        "# --- RMSE Helper for SoftImpute (No Biases) ---\n",
        "def evaluate_rmse_low_rank(U, S, V, probe_movies_mapped, probe_users_mapped, probe_ratings_true, use_gpu=False):\n",
        "    \"\"\"Computes RMSE for low-rank model X = USV^T against true ratings.\"\"\"\n",
        "    if probe_ratings_true.size == 0: return np.nan\n",
        "    xp = cp if use_gpu else np\n",
        "    try:\n",
        "        if M_movies_active == 0 or N_users_active == 0: return np.nan\n",
        "        if probe_movies_mapped.size > 0 and (probe_movies_mapped.max() >= M_movies_active or probe_movies_mapped.min() < 0): return np.nan\n",
        "        if probe_users_mapped.size > 0 and (probe_users_mapped.max() >= N_users_active or probe_users_mapped.min() < 0): return np.nan\n",
        "        U_dev = xp.asarray(U); S_dev = xp.asarray(S); V_dev = xp.asarray(V)\n",
        "        probe_movies_dev = xp.asarray(probe_movies_mapped); probe_users_dev = xp.asarray(probe_users_mapped)\n",
        "        probe_ratings_dev = xp.asarray(probe_ratings_true)\n",
        "        term2 = S_dev * V_dev[probe_users_dev, :]\n",
        "        preds_raw = xp.sum(U_dev[probe_movies_dev, :] * term2, axis=1)\n",
        "        preds_clamped = xp.clip(preds_raw, 1.0, 5.0)\n",
        "        if not xp.isfinite(preds_clamped).all(): preds_clamped = xp.nan_to_num(preds_clamped, nan=3.0)\n",
        "        if not xp.isfinite(probe_ratings_dev).all(): probe_ratings_dev = xp.nan_to_num(probe_ratings_dev)\n",
        "        mse_dev = xp.mean((preds_clamped - probe_ratings_dev)**2)\n",
        "        mse = float(cp.asnumpy(mse_dev) if use_gpu else mse_dev)\n",
        "        rmse = np.sqrt(mse) if mse >= 0 else np.nan\n",
        "    except IndexError as e: logger.error(f\"IndexError during low-rank RMSE: {e}\"); return np.nan\n",
        "    except Exception as e: logger.error(f\"Error during low-rank RMSE: {e}\"); return np.nan\n",
        "    return float(rmse) if np.isfinite(rmse) else np.nan\n",
        "\n",
        "# --- Initialization ---\n",
        "def initialize_factors_and_biases(M: int, N: int, R: int, rng: Generator, scale: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Initializes U, W, user_bias, movie_bias.\"\"\"\n",
        "    U = None; W = None; user_bias = None; movie_bias = None\n",
        "    if RANK_MPI == 0:\n",
        "        U = rng.standard_normal(size=(M, R)).astype(np.float64) * scale\n",
        "        W = rng.standard_normal(size=(R, N)).astype(np.float64) * scale\n",
        "        user_bias = np.zeros(N, dtype=np.float64)\n",
        "        movie_bias = np.zeros(M, dtype=np.float64)\n",
        "        if M >= R: U_orth, _ = np.linalg.qr(U, mode='reduced'); U = U_orth.astype(np.float64)\n",
        "        else: logger.warning(f\"M ({M}) < R ({R}). Cannot orthonormalize U.\")\n",
        "    if COMM and SIZE_MPI > 1:\n",
        "        if RANK_MPI != 0: U = np.empty((M, R), dtype=np.float64); W = np.empty((R, N), dtype=np.float64); user_bias = np.empty(N, dtype=np.float64); movie_bias = np.empty(M, dtype=np.float64)\n",
        "        COMM.Bcast(U, root=0); COMM.Bcast(W, root=0); COMM.Bcast(user_bias, root=0); COMM.Bcast(movie_bias, root=0)\n",
        "    return U, W, user_bias, movie_bias\n",
        "\n",
        "# --- Initial State Recorder ---\n",
        "def record_initial_state_biased(U, W, user_bias, movie_bias, loss_args_biased, eval_args_biased):\n",
        "    \"\"\"Computes and logs initial state for biased models.\"\"\"\n",
        "    current_loss, gU0, gW0, gBu0, gBi0 = loss_and_grad_serial_with_biases(U, W, user_bias, movie_bias, *loss_args_biased)\n",
        "    current_rmse = evaluate_rmse_with_biases(U, W, user_bias, movie_bias, *eval_args_biased)\n",
        "    gU_proj_0 = ProjTangent(U, gU0)\n",
        "    grad_norm_U_riemann = np.linalg.norm(gU_proj_0)\n",
        "    grad_norm_W = np.linalg.norm(gW0); grad_norm_Bu = np.linalg.norm(gBu0); grad_norm_Bi = np.linalg.norm(gBi0)\n",
        "    if RANK_MPI == 0: logger.info(\n",
        "        f\"Epoch 00 (Init): Loss={current_loss:.4e}, RMSE={current_rmse:.4f}, \"\n",
        "        f\"||Proj gU||={grad_norm_U_riemann:.2e}, ||gW||={grad_norm_W:.2e}, \"\n",
        "        f\"||gBu||={grad_norm_Bu:.2e}, ||gBi||={grad_norm_Bi:.2e}\"\n",
        "    )\n",
        "    if not np.isfinite(current_loss): raise ValueError(\"Initial loss is not finite.\")\n",
        "    return current_loss, current_rmse, gU0, gW0, gBu0, gBi0\n",
        "\n",
        "# --- Armijo Line Search ---\n",
        "def ArmijoLineSearchRiemannian(\n",
        "    U: np.ndarray, G_euclidean: np.ndarray, loss_args: tuple, current_loss: float,\n",
        "    lr_init: float, beta: float, sigma: float, max_ls_iter: int = 20\n",
        ") -> Tuple[float, np.ndarray, float]:\n",
        "    \"\"\"Performs Armijo line search using retraction.\"\"\"\n",
        "    lr = lr_init\n",
        "    G_proj = ProjTangent(U, G_euclidean)\n",
        "    G_proj_norm_sq = np.linalg.norm(G_proj)**2\n",
        "    if G_proj_norm_sq < 1e-14: return 0.0, U, current_loss\n",
        "    for ls_iter in range(max_ls_iter):\n",
        "        step_vec = -lr * G_proj\n",
        "        U_next = R_fn(U, step_vec) # Use chosen retraction\n",
        "        if not np.isfinite(U_next).all(): lr *= beta; continue\n",
        "        try:\n",
        "            W_ls, ub_ls, mb_ls, *rest_args = loss_args\n",
        "            loss_next, _, _, _, _ = loss_and_grad_serial_with_biases(U_next, W_ls, ub_ls, mb_ls, *rest_args)\n",
        "        except Exception as e: logger.error(f\"Armijo LS Error: {e}\"); return 0.0, U, current_loss\n",
        "        if not np.isfinite(loss_next): lr *= beta; continue\n",
        "        required_decrease = sigma * lr * G_proj_norm_sq\n",
        "        actual_decrease = current_loss - loss_next\n",
        "        if actual_decrease >= required_decrease - 1e-9: return lr, U_next, loss_next\n",
        "        lr *= beta\n",
        "        if lr < 1e-14: break\n",
        "    logger.debug(\"Armijo LS failed.\"); return 0.0, U, current_loss\n",
        "\n",
        "# --- Adaptive Stopping Check ---\n",
        "def should_stop_subproblem(G_proj, step_vec):\n",
        "    \"\"\"Return True if both criteria are already small.\"\"\"\n",
        "    grad_norm_proj = np.linalg.norm(G_proj)\n",
        "    step_norm = np.linalg.norm(step_vec)\n",
        "    stop = (grad_norm_proj < ETA_GRAD and step_norm < ETA_DIST)\n",
        "    return stop\n",
        "\n",
        "# --- Adaptive Kappa Update ---\n",
        "def update_kappa_adaptive(kappa_prev, h_hist, dist_hist, U_local,\n",
        "                          gamma=2.0, window=3,\n",
        "                          kappa_min=1e-4, kappa_max=1e12):\n",
        "    \"\"\" Adaptive kappa update using local curvature estimate. \"\"\"\n",
        "    if U_local.shape[1] == 0: return kappa_min # Handle empty matrix case\n",
        "    v = GLOBAL_RNG.standard_normal(size=(U_local.shape[1], 1)).astype(U_local.dtype)\n",
        "    v /= np.linalg.norm(v) + 1e-12\n",
        "    U_local_64 = U_local.astype(np.float64); v_64 = v.astype(np.float64)\n",
        "    lambda_max_sq = 0.0\n",
        "    for _ in range(2): # 2 power iterations on U^T U\n",
        "        Av = U_local_64.T @ (U_local_64 @ v_64)\n",
        "        lambda_max_sq = v_64.T @ Av\n",
        "        v_norm = np.linalg.norm(Av); v_64 = Av / (v_norm + 1e-12)\n",
        "    L_local = np.sqrt(max(0.0, lambda_max_sq.item()))\n",
        "    target_ratio = 0.9; target = target_ratio * L_local\n",
        "    kappa_new = np.clip(target, kappa_min, kappa_max)\n",
        "    return float(kappa_new)\n",
        "\n",
        "# --- OT Demo Helper ---\n",
        "def run_barycentre_demo(n_grid=200, reg=1e-1, rng_seed=0):\n",
        "    \"\"\" POT demo: 3 one-dimensional Gaussians -> entropic Wasserstein barycenter \"\"\"\n",
        "    if not OT_AVAILABLE: return None\n",
        "    grid = np.linspace(-8.0, 8.0, n_grid)\n",
        "    M = ot.dist(grid.reshape(-1, 1), grid.reshape(-1, 1)) ** 2\n",
        "    means = np.array([-3.0, 0.0, 3.0]); sigmas = np.array([0.5, 1.0, 0.7])\n",
        "    sources = np.vstack([np.exp(-0.5 * ((grid - m) / s) ** 2) / (s * np.sqrt(2 * np.pi)) for m, s in zip(means, sigmas)]).T\n",
        "    sources /= sources.sum(axis=0, keepdims=True)\n",
        "    bary, log = ot.bregman.barycenter(sources, M, reg, weights=None, numItermax=1000, stopThr=1e-7, log=True)\n",
        "    return {'grid': grid, 'sources': sources, 'barycenter': bary, 'log': log}\n",
        "\n",
        "\n",
        "logger.info(\"Cell 3: Model Helpers Defined.\")\n",
        "\n",
        "# ============================================================================ #\n",
        "# CELL 4: Non-Convex Solvers (SVRG, ALS, Euclidean GD) - Renumbered\n",
        "# ============================================================================ #\n",
        "logger.info(\"+++ Cell 4: Defining Non-Convex Solvers +++\")\n",
        "# --- Loss/Gradient Functions ---\n",
        "def loss_and_grad_serial_with_biases(\n",
        "    U: np.ndarray, W: np.ndarray, user_bias: np.ndarray, movie_bias: np.ndarray,\n",
        "    global_mean: float,\n",
        "    rows_idx: np.ndarray, cols_idx: np.ndarray, vals_true_centered: np.ndarray, # Centered ratings\n",
        "    n_movies_func: int, n_users_func: int, rank_func: int,\n",
        "    lambda_sq_func: float, lambda_bias_func: float\n",
        ") -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\" Computes loss and gradients for U, W, user_bias, movie_bias. \"\"\"\n",
        "    # ... (implementation from v11) ...\n",
        "    U = U.astype(np.float64, copy=False); W = W.astype(np.float64, copy=False)\n",
        "    user_bias = user_bias.astype(np.float64, copy=False); movie_bias = movie_bias.astype(np.float64, copy=False)\n",
        "    if vals_true_centered.size == 0: return 0.0, np.zeros_like(U), np.zeros_like(W), np.zeros_like(user_bias), np.zeros_like(movie_bias)\n",
        "    try:\n",
        "        W_cols = W[:, cols_idx]; U_rows = U[rows_idx, :]\n",
        "        dot_prods = np.sum(U_rows * W_cols.T, axis=1)\n",
        "        preds_residual = user_bias[cols_idx] + movie_bias[rows_idx] + dot_prods\n",
        "    except IndexError as e: logger.error(f\"Indexing error in loss_and_grad_serial_with_biases - {e}\"); raise\n",
        "    valid_mask = np.isfinite(preds_residual) & np.isfinite(vals_true_centered)\n",
        "    if not np.all(valid_mask):\n",
        "        logger.warning(f\"Filtering {np.sum(~valid_mask)} non-finite values in loss_and_grad_serial_with_biases.\")\n",
        "        rows_idx_filt = rows_idx[valid_mask]; cols_idx_filt = cols_idx[valid_mask]\n",
        "        vals_true_filt = vals_true_centered[valid_mask]; preds_filt = preds_residual[valid_mask]\n",
        "        if preds_filt.size == 0: return np.inf, np.zeros_like(U), np.zeros_like(W), np.zeros_like(user_bias), np.zeros_like(movie_bias)\n",
        "    else:\n",
        "        rows_idx_filt, cols_idx_filt, vals_true_filt, preds_filt = rows_idx, cols_idx, vals_true_centered, preds_residual\n",
        "    errors = preds_filt - vals_true_filt\n",
        "    loss_obs = 0.5 * np.sum(errors**2)\n",
        "    loss_reg_U = 0.5 * lambda_sq_func * np.sum(U**2); loss_reg_W = 0.5 * lambda_sq_func * np.sum(W**2)\n",
        "    loss_reg_bu = 0.5 * lambda_bias_func * np.sum(user_bias**2); loss_reg_bi = 0.5 * lambda_bias_func * np.sum(movie_bias**2)\n",
        "    total_loss = loss_obs + loss_reg_U + loss_reg_W + loss_reg_bu + loss_reg_bi\n",
        "    E_sparse = sparse.csr_matrix((errors, (rows_idx_filt, cols_idx_filt)), shape=(n_movies_func, n_users_func))\n",
        "    E_sparse_csc = E_sparse.tocsc()\n",
        "    grad_U = E_sparse @ W.T + lambda_sq_func * U\n",
        "    grad_W = U.T @ E_sparse_csc + lambda_sq_func * W\n",
        "    grad_user_bias = np.array(E_sparse.sum(axis=0)).flatten() + lambda_bias_func * user_bias\n",
        "    grad_movie_bias = np.array(E_sparse.sum(axis=1)).flatten() + lambda_bias_func * movie_bias\n",
        "    if not np.isfinite(grad_U).all(): grad_U = np.nan_to_num(grad_U)\n",
        "    if not np.isfinite(grad_W).all(): grad_W = np.nan_to_num(grad_W)\n",
        "    if not np.isfinite(grad_user_bias).all(): grad_user_bias = np.nan_to_num(grad_user_bias)\n",
        "    if not np.isfinite(grad_movie_bias).all(): grad_movie_bias = np.nan_to_num(grad_movie_bias)\n",
        "    if not np.isfinite(total_loss): total_loss = np.inf\n",
        "    return float(total_loss), grad_U.astype(np.float64), grad_W.astype(np.float64), grad_user_bias.astype(np.float64), grad_movie_bias.astype(np.float64)\n",
        "\n",
        "def gradient_batch_with_biases(\n",
        "    U: np.ndarray, W: np.ndarray, user_bias: np.ndarray, movie_bias: np.ndarray,\n",
        "    indices: np.ndarray, # Indices into GLOBAL triplets\n",
        "    rows_idx: np.ndarray, cols_idx: np.ndarray, vals_true_centered: np.ndarray, # Centered ratings\n",
        "    n_ratings_total: int,\n",
        "    lambda_sq_func: float, lambda_bias_func: float\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\" Computes average Euclidean gradient over a BATCH of ratings, including biases. \"\"\"\n",
        "    U = U.astype(np.float64, copy=False)\n",
        "    W = W.astype(np.float64, copy=False)\n",
        "    user_bias = user_bias.astype(np.float64, copy=False)\n",
        "    movie_bias = movie_bias.astype(np.float64, copy=False)\n",
        "    batch_size = len(indices)\n",
        "    if batch_size == 0:\n",
        "        return np.zeros_like(U), np.zeros_like(W), np.zeros_like(user_bias), np.zeros_like(movie_bias)\n",
        "\n",
        "    # Get data for the batch\n",
        "    batch_rows = rows_idx[indices]\n",
        "    batch_cols = cols_idx[indices]\n",
        "    batch_vals_centered = vals_true_centered[indices]\n",
        "\n",
        "    # Get corresponding factors and biases\n",
        "    try:\n",
        "        U_batch = U[batch_rows, :] # Shape (B, R)\n",
        "        W_batch = W[:, batch_cols] # Shape (R, B)\n",
        "        user_bias_batch = user_bias[batch_cols] # Shape (B,)\n",
        "        movie_bias_batch = movie_bias[batch_rows] # Shape (B,)\n",
        "    except IndexError as e:\n",
        "         logger.error(f\"Indexing error in gradient_batch_with_biases - {e}\")\n",
        "         raise\n",
        "\n",
        "    # Predict residual for the batch\n",
        "    preds_batch_residual = user_bias_batch + movie_bias_batch + np.sum(U_batch * W_batch.T, axis=1)\n",
        "\n",
        "    # Calculate errors for the batch\n",
        "    errors_batch = preds_batch_residual - batch_vals_centered # Shape (B,)\n",
        "\n",
        "    # Calculate gradient terms using sparse matrix approach\n",
        "    E_sparse_batch = sparse.csr_matrix((errors_batch, (batch_rows, batch_cols)),\n",
        "                                       shape=(U.shape[0], W.shape[1]))\n",
        "\n",
        "    # Average gradient over the batch\n",
        "    grad_U_batch = (E_sparse_batch @ W.T) / batch_size + lambda_sq_func * U\n",
        "    grad_W_batch = (U.T @ E_sparse_batch.tocsc()) / batch_size + lambda_sq_func * W\n",
        "\n",
        "    # Compute bias gradients (need to average errors per user/movie in batch)\n",
        "    # This requires accumulating errors per user/movie index present in the batch\n",
        "    grad_user_bias_batch = np.zeros_like(user_bias)\n",
        "    grad_movie_bias_batch = np.zeros_like(movie_bias)\n",
        "    np.add.at(grad_user_bias_batch, batch_cols, errors_batch) # Accumulate errors by user index\n",
        "    np.add.at(grad_movie_bias_batch, batch_rows, errors_batch) # Accumulate errors by movie index\n",
        "\n",
        "    grad_user_bias_batch = grad_user_bias_batch / batch_size + lambda_bias_func * user_bias\n",
        "    grad_movie_bias_batch = grad_movie_bias_batch / batch_size + lambda_bias_func * movie_bias\n",
        "\n",
        "    # Handle potential non-finite values\n",
        "    if not np.isfinite(grad_U_batch).all(): grad_U_batch = np.nan_to_num(grad_U_batch)\n",
        "    if not np.isfinite(grad_W_batch).all(): grad_W_batch = np.nan_to_num(grad_W_batch)\n",
        "    if not np.isfinite(grad_user_bias_batch).all(): grad_user_bias_batch = np.nan_to_num(grad_user_bias_batch)\n",
        "    if not np.isfinite(grad_movie_bias_batch).all(): grad_movie_bias_batch = np.nan_to_num(grad_movie_bias_batch)\n",
        "\n",
        "    return grad_U_batch.astype(np.float64), grad_W_batch.astype(np.float64), grad_user_bias_batch.astype(np.float64), grad_movie_bias_batch.astype(np.float64)\n",
        "\n",
        "# --- SVRG Solver ---\n",
        "# --- SVRG Solver with Biases ---\n",
        "def run_non_convex_svrg_with_biases(\n",
        "    R_train_coo: sparse.coo_matrix, # Contains centered ratings\n",
        "    global_mean: float,\n",
        "    probe_users_mapped: np.ndarray, # Mapped probe indices\n",
        "    probe_movies_mapped: np.ndarray,\n",
        "    probe_ratings_true: np.ndarray, # Original probe ratings\n",
        "    N_users_active: int,\n",
        "    M_movies_active: int,\n",
        "    rank_local: int,\n",
        "    n_epochs: int,\n",
        "    inner_lr: float, # Base inner learning rate\n",
        "    batch_size: int,\n",
        "    lam_sq: float,\n",
        "    lam_bias: float,\n",
        "    rng: Generator,\n",
        "    init_scale: float = INIT_SCALE_NON_CONVEX,\n",
        "    max_grad_norm: float = GRAD_CLIP_THRESHOLD\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"\n",
        "    Runs SVRG for non-convex UW factorization including bias terms.\n",
        "    Uses decaying LR and gradient clipping.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Non-Convex SVRG Solver with Biases...\")\n",
        "    # Initialize factors and biases\n",
        "    U, W, user_bias, movie_bias = initialize_factors_and_biases(\n",
        "        M_movies_active, N_users_active, rank_local, rng, init_scale\n",
        "    )\n",
        "\n",
        "    hist_loss = []\n",
        "    hist_rmse = []\n",
        "    hist_time = []\n",
        "    hist_gU_norm, hist_gW_norm, hist_gBu_norm, hist_gBi_norm = [], [], [], []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use mapped indices and centered ratings for training\n",
        "    train_rows = R_train_coo.row\n",
        "    train_cols = R_train_coo.col\n",
        "    train_vals_centered = R_train_coo.data\n",
        "    n_ratings_total = R_train_coo.nnz\n",
        "\n",
        "    if n_ratings_total == 0:\n",
        "        logger.error(\"No training ratings available.\")\n",
        "        return {'loss': [], 'rmse': [], 'time': [], 'gU_norm': [], 'gW_norm': [], 'gBu_norm': [], 'gBi_norm': [], 'U': None, 'W': None, 'bu': None, 'bi': None}\n",
        "\n",
        "    # Initial evaluation\n",
        "    try:\n",
        "        loss0, gU0, gW0, gBu0, gBi0 = loss_and_grad_serial_with_biases(\n",
        "            U, W, user_bias, movie_bias, global_mean,\n",
        "            train_rows, train_cols, train_vals_centered,\n",
        "            M_movies_active, N_users_active, rank_local, lam_sq, lam_bias\n",
        "        )\n",
        "        rmse0 = evaluate_rmse_with_biases(\n",
        "            U, W, user_bias, movie_bias, global_mean,\n",
        "            probe_users_mapped, probe_movies_mapped, probe_ratings_true\n",
        "        )\n",
        "        hist_loss.append(loss0)\n",
        "        hist_rmse.append(rmse0)\n",
        "        hist_time.append(time.time() - start_time)\n",
        "        hist_gU_norm.append(np.linalg.norm(gU0))\n",
        "        hist_gW_norm.append(np.linalg.norm(gW0))\n",
        "        hist_gBu_norm.append(np.linalg.norm(gBu0))\n",
        "        hist_gBi_norm.append(np.linalg.norm(gBi0))\n",
        "        logger.info(\n",
        "            f\"Epoch 00 (Init): Loss={loss0:.4e}, RMSE={rmse0:.4f}, \"\n",
        "            f\"||gU||={hist_gU_norm[-1]:.2e}, ||gW||={hist_gW_norm[-1]:.2e}, \"\n",
        "            f\"||gBu||={hist_gBu_norm[-1]:.2e}, ||gBi||={hist_gBi_norm[-1]:.2e}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during initial evaluation: {e}\", exc_info=True)\n",
        "        return {'loss': [], 'rmse': [], 'time': [], 'gU_norm': [], 'gW_norm': [], 'gBu_norm': [], 'gBi_norm': [], 'U': None, 'W': None, 'bu': None, 'bi': None}\n",
        "\n",
        "    # Main SVRG Loop\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        logger.info(f\"--- Starting Epoch {epoch:02d} ---\")\n",
        "        # --- Use Exponential Decay for Learning Rate (FIX 4) ---\n",
        "        lr_epoch = inner_lr * (0.9**(epoch - 1)) # Exponential decay\n",
        "        logger.info(f\"Using lr = {lr_epoch:.2e} this epoch\")\n",
        "        # ---------------------------------------------\n",
        "\n",
        "        # Compute anchor gradient\n",
        "        logger.info(f\"Epoch {epoch:02d}: Computing anchor gradient...\")\n",
        "        anchor_start_time = time.time()\n",
        "        try:\n",
        "            loss_anchor, gU_anchor, gW_anchor, gBu_anchor, gBi_anchor = loss_and_grad_serial_with_biases(\n",
        "                U, W, user_bias, movie_bias, global_mean,\n",
        "                train_rows, train_cols, train_vals_centered,\n",
        "                M_movies_active, N_users_active, rank_local, lam_sq, lam_bias\n",
        "            )\n",
        "            logger.info(f\"Epoch {epoch:02d}: Anchor gradient computed in {time.time() - anchor_start_time:.2f}s.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error computing anchor gradient at epoch {epoch}: {e}\")\n",
        "            break\n",
        "\n",
        "        U_epoch_start, W_epoch_start = U.copy(), W.copy()\n",
        "        user_bias_epoch_start, movie_bias_epoch_start = user_bias.copy(), movie_bias.copy()\n",
        "\n",
        "        # Inner loop\n",
        "        # --- Use Full Inner Pass (FIX 5) ---\n",
        "        num_inner_steps = max(1, (n_ratings_total // batch_size) // SVRG_INNER_STEPS_DIVISOR)\n",
        "        logger.info(f\"Epoch {epoch:02d}: Starting inner loop with {num_inner_steps} steps...\")\n",
        "        inner_loop_start_time = time.time()\n",
        "\n",
        "        for inner_step in range(num_inner_steps):\n",
        "            batch_indices = rng.choice(n_ratings_total, size=batch_size, replace=False)\n",
        "            try:\n",
        "                gU_curr, gW_curr, gBu_curr, gBi_curr = gradient_batch_with_biases(\n",
        "                    U, W, user_bias, movie_bias, batch_indices,\n",
        "                    train_rows, train_cols, train_vals_centered,\n",
        "                    n_ratings_total, lam_sq, lam_bias)\n",
        "                gU_anch, gW_anch, gBu_anch, gBi_anch = gradient_batch_with_biases(\n",
        "                    U_epoch_start, W_epoch_start, user_bias_epoch_start, movie_bias_epoch_start,\n",
        "                    batch_indices, train_rows, train_cols, train_vals_centered,\n",
        "                    n_ratings_total, lam_sq, lam_bias)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error computing stochastic gradient: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Variance-reduced gradients\n",
        "            gU_vr = gU_curr - gU_anch + gU_anchor\n",
        "            gW_vr = gW_curr - gW_anch + gW_anchor\n",
        "            gBu_vr = gBu_curr - gBu_anch + gBu_anchor\n",
        "            gBi_vr = gBi_curr - gBi_anch + gBi_anchor\n",
        "\n",
        "            # Gradient clipping\n",
        "            gU_norm = np.linalg.norm(gU_vr); gW_norm = np.linalg.norm(gW_vr)\n",
        "            gBu_norm = np.linalg.norm(gBu_vr); gBi_norm = np.linalg.norm(gBi_vr)\n",
        "            if gU_norm > max_grad_norm: gU_vr *= (max_grad_norm / gU_norm)\n",
        "            if gW_norm > max_grad_norm: gW_vr *= (max_grad_norm / gW_norm)\n",
        "            if gBu_norm > max_grad_norm: gBu_vr *= (max_grad_norm / gBu_norm)\n",
        "            if gBi_norm > max_grad_norm: gBi_vr *= (max_grad_norm / gBi_norm)\n",
        "\n",
        "            # Update factors and biases\n",
        "            U -= lr_epoch * gU_vr\n",
        "            W -= lr_epoch * gW_vr\n",
        "            user_bias -= lr_epoch * gBu_vr\n",
        "            movie_bias -= lr_epoch * gBi_vr\n",
        "\n",
        "            if (inner_step + 1) % 5000 == 0: # Log less frequently for full inner pass\n",
        "                logger.info(f\"Epoch {epoch:02d}: Inner step {inner_step+1}/{num_inner_steps} done.\")\n",
        "\n",
        "        logger.info(f\"Epoch {epoch:02d}: Inner loop finished in {time.time() - inner_loop_start_time:.2f}s.\")\n",
        "\n",
        "        # Evaluate after epoch\n",
        "        logger.info(f\"Epoch {epoch:02d}: Evaluating loss and RMSE...\")\n",
        "        eval_start_time = time.time()\n",
        "        try:\n",
        "            loss_k, gU_k, gW_k, gBu_k, gBi_k = loss_and_grad_serial_with_biases(\n",
        "                U, W, user_bias, movie_bias, global_mean,\n",
        "                train_rows, train_cols, train_vals_centered,\n",
        "                M_movies_active, N_users_active, rank_local, lam_sq, lam_bias\n",
        "            )\n",
        "            if not np.isfinite(loss_k):\n",
        "                logger.error(f\"Epoch {epoch:02d}: Loss became non-finite ({loss_k}). Stopping.\")\n",
        "                hist_loss.append(np.nan); hist_rmse.append(np.nan); hist_time.append(time.time() - start_time)\n",
        "                hist_gU_norm.append(np.nan); hist_gW_norm.append(np.nan); hist_gBu_norm.append(np.nan); hist_gBi_norm.append(np.nan)\n",
        "                break\n",
        "\n",
        "            rmse_k = evaluate_rmse_with_biases(\n",
        "                U, W, user_bias, movie_bias, global_mean,\n",
        "                probe_users_mapped, probe_movies_mapped, probe_ratings_true\n",
        "            )\n",
        "            hist_loss.append(loss_k); hist_rmse.append(rmse_k)\n",
        "            hist_time.append(time.time() - start_time)\n",
        "            hist_gU_norm.append(np.linalg.norm(gU_k)); hist_gW_norm.append(np.linalg.norm(gW_k))\n",
        "            hist_gBu_norm.append(np.linalg.norm(gBu_k)); hist_gBi_norm.append(np.linalg.norm(gBi_k))\n",
        "\n",
        "            logger.info(f\"Epoch {epoch:02d}: Eval done in {time.time() - eval_start_time:.2f}s. \")\n",
        "            logger.info(\n",
        "                f\"Loss={loss_k:.4e}, RMSE={rmse_k:.4f}, \"\n",
        "                f\"||gU||={hist_gU_norm[-1]:.2e}, ||gW||={hist_gW_norm[-1]:.2e}, \"\n",
        "                f\"||gBu||={hist_gBu_norm[-1]:.2e}, ||gBi||={hist_gBi_norm[-1]:.2e}\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation at epoch {epoch}: {e}\", exc_info=True)\n",
        "            hist_loss.append(np.nan); hist_rmse.append(np.nan); hist_time.append(time.time() - start_time)\n",
        "            hist_gU_norm.append(np.nan); hist_gW_norm.append(np.nan); hist_gBu_norm.append(np.nan); hist_gBi_norm.append(np.nan)\n",
        "            break\n",
        "\n",
        "        logger.info(f\"--- Epoch {epoch:02d} finished in {time.time() - epoch_start_time:.2f}s ---\")\n",
        "\n",
        "    logger.info(\"Non-Convex SVRG Solver with Biases Finished.\")\n",
        "    return {\n",
        "        'loss': hist_loss, 'rmse': hist_rmse, 'time': hist_time,\n",
        "        'gU_norm': hist_gU_norm, 'gW_norm': hist_gW_norm,\n",
        "        'gBu_norm': hist_gBu_norm, 'gBi_norm': hist_gBi_norm,\n",
        "        'U': U, 'W': W, 'bu': user_bias, 'bi': movie_bias\n",
        "    }\n",
        "\n",
        "\n",
        "# --- ALS Solver ---\n",
        "\n",
        "def W_closed_efficient(U, N_users, N_movies, user_indices=None):\n",
        "    # Solves for W for a subset of users (local computation)\n",
        "    U = U.astype(np.float32, copy=False);\n",
        "    target_users = user_indices if user_indices is not None else user_data_arrays.keys()\n",
        "    W_subset = {} # Use dict if only computing for subset\n",
        "    I_r_lam_sq = (LAM_SQ * I_r).astype(np.float32) # lambda^2 * I\n",
        "\n",
        "    for u in target_users:\n",
        "        if u not in user_data_arrays: continue\n",
        "        data = user_data_arrays[u]\n",
        "        movie_indices = data['movies']; rs_t = data['rs']\n",
        "        if movie_indices.size == 0: continue\n",
        "        # Check bounds before indexing U\n",
        "        if movie_indices.max() >= U.shape[0] or movie_indices.min() < 0:\n",
        "            # if RANK_MPI == 0: print(f\"Warning: Invalid movie indices for user {u}. Skipping.\")\n",
        "            continue\n",
        "        U_k = U[movie_indices, :]\n",
        "        A = U_k.T @ U_k + I_r_lam_sq\n",
        "        B = U_k.T @ rs_t\n",
        "        A = A.astype(np.float32); B = B.astype(np.float32)\n",
        "        try:\n",
        "            w_u = np.linalg.solve(A.astype(np.float64), B.astype(np.float64)).astype(np.float32)\n",
        "        except np.linalg.LinAlgError:\n",
        "            # if RANK_MPI == 0: print(f\"Warning: np.linalg.solve failed for user {u}. Using pseudo-inverse.\")\n",
        "            try:\n",
        "                w_u = (np.linalg.pinv(A.astype(np.float64)) @ B.astype(np.float64)).astype(np.float32)\n",
        "            except np.linalg.LinAlgError:\n",
        "                if RANK_MPI == 0: print(f\"ERROR: Pseudo-inverse also failed for user {u}. Returning zero vector.\")\n",
        "                w_u = np.zeros(RANK, dtype=np.float32) # Return zero vector if fails completely\n",
        "            except Exception as e_pinv:\n",
        "                 if RANK_MPI == 0: print(f\"ERROR: Unknown error in pseudo-inverse for user {u}: {e_pinv}. Returning zero vector.\")\n",
        "                 w_u = np.zeros(RANK, dtype=np.float32)\n",
        "\n",
        "        if user_indices is not None:\n",
        "            W_subset[u] = w_u\n",
        "        else:\n",
        "            if 'W' not in locals(): W = np.zeros((RANK, N_users), dtype=np.float32)\n",
        "            if 0 <= u < W.shape[1]: # Check user index bound for W\n",
        "                 W[:, u] = w_u\n",
        "            # else: # This shouldn't happen if N_users is correct\n",
        "            #    if RANK_MPI == 0: print(f\"Warning: User index {u} out of bounds for W (shape {W.shape}).\")\n",
        "\n",
        "\n",
        "    if user_indices is not None:\n",
        "        return W_subset # Return dict\n",
        "    else:\n",
        "        if 'W' not in locals():\n",
        "            # if RANK_MPI == 0: print(\"Warning: W_closed_efficient called with no active users? Returning empty W.\")\n",
        "            return np.zeros((RANK, N_users), dtype=np.float32)\n",
        "        # W should be filled now\n",
        "        if not np.isfinite(W).all():\n",
        "            if RANK_MPI == 0: print(\"Warning: Non-finite values found in computed W matrix. Clamping.\")\n",
        "            W = np.nan_to_num(W, nan=0.0, posinf=0.0, neginf=0.0) # Clamp non-finite to zero\n",
        "        assert W.shape == (RANK, N_users);\n",
        "        return W # Return full W matrix\n",
        "\n",
        "\n",
        "def update_user_factors(\n",
        "    R_train_coo_csc: sparse.csc_matrix, # Centered ratings, CSC format\n",
        "    U: np.ndarray,\n",
        "    user_bias: np.ndarray,\n",
        "    movie_bias: np.ndarray,\n",
        "    lambda_sq: float,\n",
        "    rank: int,\n",
        "    N_users: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Solves for W (user factors) fixing U and biases.\"\"\"\n",
        "    M = U.shape[0]\n",
        "    W = np.zeros((rank, N_users), dtype=np.float64)\n",
        "    # Precompute U^T U + lambda*I (used in the denominator)\n",
        "    # Note: This is used inside the loop per user based on specific movies U_j\n",
        "    # UtU = U.T @ U + lambda_sq * np.eye(rank, dtype=np.float64) # Can't precompute fully\n",
        "\n",
        "    for j in range(N_users):\n",
        "        # Find ratings for user j\n",
        "        start_idx = R_train_coo_csc.indptr[j]\n",
        "        end_idx = R_train_coo_csc.indptr[j+1]\n",
        "        if start_idx == end_idx: # No ratings for this user\n",
        "            continue\n",
        "\n",
        "        movie_indices = R_train_coo_csc.indices[start_idx:end_idx]\n",
        "        ratings_centered = R_train_coo_csc.data[start_idx:end_idx]\n",
        "\n",
        "        U_j = U[movie_indices, :] # Movies rated by user j (n_j x R)\n",
        "\n",
        "        # Adjust ratings by movie bias: r_ij - mu - b_i\n",
        "        adjusted_ratings = ratings_centered - movie_bias[movie_indices]\n",
        "\n",
        "        # Calculate A = U_j^T U_j + lambda*I\n",
        "        A = U_j.T @ U_j + lambda_sq * np.eye(rank, dtype=np.float64)\n",
        "\n",
        "        # Calculate b = U_j^T * adjusted_ratings\n",
        "        b = U_j.T @ adjusted_ratings\n",
        "\n",
        "        try:\n",
        "            W[:, j] = np.linalg.solve(A, b)\n",
        "        except np.linalg.LinAlgError:\n",
        "            logger.warning(f\"ALS: Solve failed for user {j}, using pseudo-inverse.\")\n",
        "            try:\n",
        "                W[:, j] = np.linalg.pinv(A) @ b\n",
        "            except Exception as e_pinv:\n",
        "                 logger.error(f\"ALS: Pseudo-inverse failed for user {j}: {e_pinv}. Setting W_j to zero.\")\n",
        "                 W[:, j] = 0.0 # Set to zero vector\n",
        "\n",
        "    return W.astype(np.float64)\n",
        "\n",
        "def update_movie_factors(\n",
        "    R_train_coo_csr: sparse.csr_matrix, # Centered ratings, CSR format\n",
        "    W: np.ndarray,\n",
        "    user_bias: np.ndarray,\n",
        "    movie_bias: np.ndarray,\n",
        "    lambda_sq: float,\n",
        "    rank: int,\n",
        "    M_movies: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Solves for U (movie factors) fixing W and biases.\"\"\"\n",
        "    N = W.shape[1]\n",
        "    U = np.zeros((M_movies, rank), dtype=np.float64)\n",
        "    # Precompute W W^T + lambda*I (used in the denominator)\n",
        "    # Note: This is used inside the loop per movie based on specific users W_i\n",
        "    # WtW = W @ W.T + lambda_sq * np.eye(rank, dtype=np.float64) # Can't precompute fully\n",
        "\n",
        "    for i in range(M_movies):\n",
        "        # Find ratings for movie i\n",
        "        start_idx = R_train_coo_csr.indptr[i]\n",
        "        end_idx = R_train_coo_csr.indptr[i+1]\n",
        "        if start_idx == end_idx: # No ratings for this movie\n",
        "            continue\n",
        "\n",
        "        user_indices = R_train_coo_csr.indices[start_idx:end_idx]\n",
        "        ratings_centered = R_train_coo_csr.data[start_idx:end_idx]\n",
        "\n",
        "        W_i = W[:, user_indices] # Users who rated movie i (R x n_i)\n",
        "\n",
        "        # Adjust ratings by user bias: r_ij - mu - b_u\n",
        "        adjusted_ratings = ratings_centered - user_bias[user_indices]\n",
        "\n",
        "        # Calculate A = W_i W_i^T + lambda*I\n",
        "        A = W_i @ W_i.T + lambda_sq * np.eye(rank, dtype=np.float64)\n",
        "\n",
        "        # Calculate b = W_i * adjusted_ratings\n",
        "        b = W_i @ adjusted_ratings\n",
        "\n",
        "        try:\n",
        "            U[i, :] = np.linalg.solve(A, b)\n",
        "        except np.linalg.LinAlgError:\n",
        "             logger.warning(f\"ALS: Solve failed for movie {i}, using pseudo-inverse.\")\n",
        "             try:\n",
        "                 U[i, :] = np.linalg.pinv(A) @ b\n",
        "             except Exception as e_pinv:\n",
        "                 logger.error(f\"ALS: Pseudo-inverse failed for movie {i}: {e_pinv}. Setting U_i to zero.\")\n",
        "                 U[i, :] = 0.0 # Set to zero vector\n",
        "\n",
        "    return U.astype(np.float64)\n",
        "\n",
        "\n",
        "def update_biases(\n",
        "    R_train_coo: sparse.coo_matrix, # Centered ratings\n",
        "    U: np.ndarray,\n",
        "    W: np.ndarray,\n",
        "    user_bias: np.ndarray,\n",
        "    movie_bias: np.ndarray,\n",
        "    global_mean: float,\n",
        "    lambda_bias: float,\n",
        "    N_users: int,\n",
        "    M_movies: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Updates user and movie biases based on current residuals.\"\"\"\n",
        "    new_user_bias = np.zeros_like(user_bias)\n",
        "    new_movie_bias = np.zeros_like(movie_bias)\n",
        "    user_counts = np.zeros_like(user_bias)\n",
        "    movie_counts = np.zeros_like(movie_bias)\n",
        "\n",
        "    # Calculate residuals: r_ij - mu - U_i^T W_j\n",
        "    rows, cols, vals_centered = R_train_coo.row, R_train_coo.col, R_train_coo.data\n",
        "    dot_prods = np.array([np.dot(U[r, :], W[:, c]) for r, c in zip(rows, cols)], dtype=np.float64)\n",
        "    residuals = vals_centered - dot_prods # Residual = (r_ij - mu) - U_i^T W_j\n",
        "\n",
        "    # Update user biases: b_u = sum(residual - b_i) / (count + lambda_bias)\n",
        "    np.add.at(new_user_bias, cols, residuals - movie_bias[rows])\n",
        "    np.add.at(user_counts, cols, 1)\n",
        "    new_user_bias = new_user_bias / (user_counts + lambda_bias + 1e-9) # Add epsilon for stability\n",
        "\n",
        "    # Update movie biases: b_i = sum(residual - b_u) / (count + lambda_bias)\n",
        "    np.add.at(new_movie_bias, rows, residuals - new_user_bias[cols]) # Use updated user bias\n",
        "    np.add.at(movie_counts, rows, 1)\n",
        "    new_movie_bias = new_movie_bias / (movie_counts + lambda_bias + 1e-9) # Add epsilon for stability\n",
        "\n",
        "    return new_user_bias.astype(np.float64), new_movie_bias.astype(np.float64)\n",
        "\n",
        "def run_als_with_biases(\n",
        "    R_train_coo: sparse.coo_matrix, # Centered ratings\n",
        "    global_mean: float,\n",
        "    probe_users_mapped: np.ndarray,\n",
        "    probe_movies_mapped: np.ndarray,\n",
        "    probe_ratings_true: np.ndarray,\n",
        "    N_users_active: int,\n",
        "    M_movies_active: int,\n",
        "    rank_local: int,\n",
        "    n_iters: int, # Max iterations\n",
        "    lam_sq: float,\n",
        "    lam_bias: float,\n",
        "    rng: Generator,\n",
        "    init_scale: float = INIT_SCALE_NON_CONVEX,\n",
        "    tol: float = ALS_TOL\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"Runs Alternating Least Squares with biases.\"\"\"\n",
        "    logger.info(\"Starting ALS Solver with Biases...\")\n",
        "    U, W, user_bias, movie_bias = initialize_factors_and_biases(\n",
        "        M_movies_active, N_users_active, rank_local, rng, init_scale\n",
        "    )\n",
        "\n",
        "    hist_loss = [] # Loss not typically tracked directly in ALS, focus on RMSE\n",
        "    hist_rmse = []\n",
        "    hist_time = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    last_rmse = np.inf\n",
        "\n",
        "    # Precompute sparse matrix formats for efficiency\n",
        "    R_train_csc = R_train_coo.tocsc()\n",
        "    R_train_csr = R_train_coo.tocsr()\n",
        "\n",
        "    for k_iter in range(1, n_iters + 1):\n",
        "        iter_start_time = time.time()\n",
        "        logger.info(f\"--- Starting ALS Iteration {k_iter:02d} ---\")\n",
        "\n",
        "        # Update user factors (W)\n",
        "        logger.debug(f\"Iter {k_iter}: Updating user factors (W)...\")\n",
        "        W = update_user_factors(R_train_csc, U, user_bias, movie_bias, lam_sq, rank_local, N_users_active)\n",
        "\n",
        "        # Update movie factors (U)\n",
        "        logger.debug(f\"Iter {k_iter}: Updating movie factors (U)...\")\n",
        "        U = update_movie_factors(R_train_csr, W, user_bias, movie_bias, lam_sq, rank_local, M_movies_active)\n",
        "\n",
        "        # Update biases\n",
        "        logger.debug(f\"Iter {k_iter}: Updating biases...\")\n",
        "        user_bias, movie_bias = update_biases(R_train_coo, U, W, user_bias, movie_bias, global_mean, lam_bias, N_users_active, M_movies_active)\n",
        "\n",
        "        # Evaluate RMSE\n",
        "        logger.debug(f\"Iter {k_iter}: Evaluating RMSE...\")\n",
        "        current_rmse = evaluate_rmse_with_biases(\n",
        "            U, W, user_bias, movie_bias, global_mean,\n",
        "            probe_users_mapped, probe_movies_mapped, probe_ratings_true\n",
        "        )\n",
        "        current_time = time.time() - start_time\n",
        "        hist_rmse.append(current_rmse)\n",
        "        hist_time.append(current_time)\n",
        "\n",
        "        iter_time = time.time() - iter_start_time\n",
        "        logger.info(f\"Iter {k_iter:02d}: RMSE = {current_rmse:.6f} (Time: {iter_time:.2f}s)\")\n",
        "\n",
        "        # Check convergence\n",
        "        if abs(last_rmse - current_rmse) < tol:\n",
        "            logger.info(f\"ALS converged at iteration {k_iter} (RMSE change < {tol})\")\n",
        "            break\n",
        "        last_rmse = current_rmse\n",
        "\n",
        "    logger.info(\"ALS Solver with Biases Finished.\")\n",
        "    return {\n",
        "        'loss': [], # ALS doesn't typically track the combined loss easily\n",
        "        'rmse': hist_rmse,\n",
        "        'time': hist_time,\n",
        "        'U': U, 'W': W, 'bu': user_bias, 'bi': movie_bias\n",
        "    }\n",
        "\n",
        "# --- Stochastic Gradient Single User (NEW - for SARAH/SPIDER) ---\n",
        "def stochastic_gradient_single_user(U, user_idx, N_users, N_movies, loss_args):\n",
        "    \"\"\" Computes the UNSCALED gradient component d L_user_idx / dU for a single user. \"\"\"\n",
        "    # Unpack loss_args (assumes structure matches loss_and_grad_serial_with_biases)\n",
        "    global_mean, rows_idx, cols_idx, vals_true_centered, _, _, rank_func, lambda_sq_func, lambda_bias_func = loss_args\n",
        "    M, R = U.shape\n",
        "    G_user = np.zeros_like(U, dtype=np.float32)\n",
        "    if user_idx not in user_data_arrays: return G_user # Use precomputed user_data_arrays\n",
        "\n",
        "    W_user_dict = W_closed_efficient(U, N_users, N_movies, user_indices=[user_idx]) # Recompute W for this user\n",
        "    if user_idx not in W_user_dict: return G_user\n",
        "\n",
        "    w_u = W_user_dict[user_idx]\n",
        "    user_data = user_data_arrays[user_idx]\n",
        "    movie_indices = user_data['movies']; rs_t = user_data['rs'] # rs_t are original ratings here\n",
        "    if movie_indices.size == 0: return G_user\n",
        "    if movie_indices.max() >= M or movie_indices.min() < 0: return G_user # Return zero grad if invalid index\n",
        "\n",
        "    # Need centered ratings and biases for gradient calculation\n",
        "    # Recompute biases? Or assume they are passed implicitly? Assume passed via loss_args implicitly (not ideal)\n",
        "    # This function signature needs alignment with how biases are handled if used by SARAH/SPIDER\n",
        "    # For now, approximate using centered ratings and current factors\n",
        "    # This needs refinement if SARAH/SPIDER are primary focus\n",
        "    ratings_centered_user = rs_t - global_mean # Approximate centering\n",
        "\n",
        "    U_k = U[movie_indices, :]\n",
        "    # Need bias terms here for correct error calculation\n",
        "    # Placeholder: Calculate error without biases for now\n",
        "    preds_k_dot = U_k @ w_u\n",
        "    err_k = preds_k_dot - ratings_centered_user # Error against centered rating\n",
        "\n",
        "    grad_vals_k = err_k # Simplified grad without prox term from loss_and_grad\n",
        "    term_k = grad_vals_k.reshape(-1, 1) * w_u.reshape(1, -1)\n",
        "    np.add.at(G_user, movie_indices, term_k.astype(np.float32))\n",
        "    # Add regularization gradient for U rows involved\n",
        "    G_user[movie_indices, :] += lambda_sq_func * U_k\n",
        "\n",
        "    if not np.isfinite(G_user).all():\n",
        "        G_user = np.nan_to_num(G_user, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    assert G_user.shape == U.shape\n",
        "    return G_user\n",
        "# --- Euclidean GD Solver (NEW from long.txt, adapted for biases) ---\n",
        "def run_euclidean_gd(\n",
        "    R_train_coo, global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true,\n",
        "    N_users_active, M_movies_active, rank_local, n_iters,\n",
        "    lam_sq, lam_bias, rng, init_scale=INIT_SCALE_NON_CONVEX, lr=1e-7 # Use specific LR\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"Runs Vanilla Euclidean GD with biases.\"\"\"\n",
        "    if RANK_MPI == 0: logger.info(f\"\\n+++ Running Vanilla Euclidean GD (LR={lr:.1e}) +++\")\n",
        "    U_euc, W_euc, user_bias, movie_bias = initialize_factors_and_biases(M_movies_active, N_users_active, rank_local, rng, init_scale)\n",
        "    # Note: Euclidean GD doesn't require U to be orthonormal, so we use the direct output\n",
        "\n",
        "    hist_loss, hist_grad, hist_rmse, hist_time = [], [], [], []; t_start = time.time();\n",
        "    loss_args_biased = (global_mean, R_train_coo.row, R_train_coo.col, R_train_coo.data, M_movies_active, N_users_active, rank_local, lam_sq, lam_bias)\n",
        "    eval_args_biased = (global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true)\n",
        "\n",
        "    try:\n",
        "        current_loss, current_rmse, gU_k, gW_k, gBu_k, gBi_k = record_initial_state_biased(U_euc, W_euc, user_bias, movie_bias, loss_args_biased, eval_args_biased)\n",
        "        grad_norm_k = np.linalg.norm(gU_k) # Use Euclidean norm for U gradient\n",
        "    except Exception as e:\n",
        "        if RANK_MPI == 0: print(f\"  ERROR during initial state recording for Euclidean GD: {e}\")\n",
        "        return {'loss': [], 'grad_norm': [], 'rmse': [], 'time': []}\n",
        "\n",
        "    if RANK_MPI == 0: hist_loss.append(current_loss); hist_grad.append(grad_norm_k); hist_rmse.append(current_rmse); hist_time.append(time.time() - t_start)\n",
        "\n",
        "    if RANK_MPI == 0: logger.info(\"\\n  Starting Euclidean GD iterations...\")\n",
        "    for k in range(n_iters):\n",
        "        iter_t0 = time.time();\n",
        "        # --- inside your Euclidean-GD loop ---\n",
        "        if grad_norm_k < 1e-6:\n",
        "            if RANK_MPI == 0:\n",
        "                logger.info(f\"EucGD converged at iter {k}\")   # or print(...)\n",
        "            break\n",
        "\n",
        "\n",
        "        # Simple Euclidean gradient step for all variables\n",
        "        U_euc -= lr * gU_k\n",
        "        W_euc -= lr * gW_k\n",
        "        user_bias -= lr * gBu_k\n",
        "        movie_bias -= lr * gBi_k\n",
        "\n",
        "        if not (np.isfinite(U_euc).all() and np.isfinite(W_euc).all()):\n",
        "            if RANK_MPI == 0: print(f\"EucGD Warning: Non-finite factors at iter {k+1}\"); break\n",
        "\n",
        "        try:\n",
        "            current_loss, gU_k, gW_k, gBu_k, gBi_k = loss_and_grad_serial_with_biases(U_euc, W_euc, user_bias, movie_bias, *loss_args_biased)\n",
        "            current_rmse = evaluate_rmse_with_biases(U_euc, W_euc, user_bias, movie_bias, *eval_args_biased)\n",
        "            grad_norm_k = np.linalg.norm(gU_k) # Euclidean norm\n",
        "            if not (np.isfinite(current_loss) and np.isfinite(gU_k).all() and (np.isnan(current_rmse) or np.isfinite(current_rmse))):\n",
        "                if RANK_MPI == 0: print(f\"EucGD Warning: Non-finite values encountered iter {k+1}.\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            if RANK_MPI == 0: print(f\"EucGD Error during iteration {k+1}: {e}\")\n",
        "            break\n",
        "\n",
        "        if RANK_MPI == 0:\n",
        "            hist_loss.append(current_loss); hist_grad.append(grad_norm_k); hist_rmse.append(current_rmse); hist_time.append(time.time() - t_start)\n",
        "            if k % 5 == 0 or k == n_iters - 1: print(f\"  EucGD Iter {k+1:02d} | Loss: {current_loss:.3e} | GradNorm: {grad_norm_k:.3e} | RMSE: {current_rmse:.4f} | Time: {time.time()-iter_t0:.2f}s\")\n",
        "\n",
        "    if RANK_MPI == 0: logger.info(f\"EucGD finished in {time.time()-t_start:.2f}s\");\n",
        "    return {'loss': hist_loss, 'grad_norm': hist_grad, 'rmse': hist_rmse, 'time': hist_time, 'U': U_euc, 'W': W_euc, 'bu': user_bias, 'bi': movie_bias}\n",
        "\n",
        "\n",
        "# ============================================================================ #\n",
        "# CELL 5: Riemannian Solvers (RGD, RAGD, Catalyst, DANE) - Renumbered\n",
        "# ============================================================================ #\n",
        "logger.info(\"+++ Cell 5: Defining Riemannian Solvers +++\")\n",
        "# --- Stochastic Solvers (SARAH, SPIDER) ---\n",
        "\n",
        "def run_soft_impute_efficient(\n",
        "    R_train_coo_orig: sparse.coo_matrix, # Original ratings, mapped indices\n",
        "    probe_users_mapped: np.ndarray,\n",
        "    probe_movies_mapped: np.ndarray,\n",
        "    probe_ratings_true: np.ndarray, # Original probe ratings\n",
        "    N_users_active: int,\n",
        "    M_movies_active: int,\n",
        "    n_iters: int,\n",
        "    lambda_reg: float,\n",
        "    k_rank: int, # Initial rank guess / cap for SVD\n",
        "    tol: float,\n",
        "    rng: Generator\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\" Solves convex problem using efficient Soft-Impute with LinearOperator SVD. \"\"\"\n",
        "    logger.info(\"Starting Efficient Convex Soft-Impute Solver (CPU)...\")\n",
        "    use_gpu = False # Force CPU as LinearOperator uses SciPy\n",
        "\n",
        "    # Prepare necessary sparse formats of original ratings\n",
        "    R_orig_csr = R_train_coo_orig.tocsr()\n",
        "    R_orig_csc = R_train_coo_orig.tocsc()\n",
        "    # Create Omega mask (1s where ratings exist)\n",
        "    omega_mask_csr = R_orig_csr.copy(); omega_mask_csr.data[:] = 1\n",
        "    omega_mask_csc = omega_mask_csr.tocsc()\n",
        "\n",
        "    # Initialize factors U, S, V\n",
        "    initial_k = max(1, min(k_rank, M_movies_active, N_users_active))\n",
        "    U = rng.standard_normal(size=(M_movies_active, initial_k)).astype(np.float64) * 0.01\n",
        "    S = np.zeros(initial_k, dtype=np.float64) # Start with S=0 -> Xk=0 initially\n",
        "    V = rng.standard_normal(size=(N_users_active, initial_k)).astype(np.float64) * 0.01\n",
        "    if N_users_active >= initial_k: V, _ = np.linalg.qr(V, mode='reduced') # Orthonormalize V initially\n",
        "\n",
        "    U_old, S_old, V_old = U.copy(), S.copy(), V.copy()\n",
        "    hist_loss, hist_rmse, hist_time, hist_rank = [], [], [], []\n",
        "    start_time = time.time()\n",
        "    current_svd_k = initial_k # Rank for svds call\n",
        "\n",
        "    for k_iter in range(1, n_iters + 1):\n",
        "        iter_start_time = time.time()\n",
        "        logger.info(f\"--- Starting SoftImpute Iteration {k_iter:02d} ---\")\n",
        "\n",
        "        # Define Linear Operator for Z = P_Omega(R_orig) + P_Omega_Complement(USV^T)\n",
        "        Z_op = ImplicitFillOperator(R_orig_csr, R_orig_csc, omega_mask_csr, omega_mask_csc, U, S, V, (M_movies_active, N_users_active))\n",
        "\n",
        "        # Perform SVD using the LinearOperator\n",
        "        logger.debug(f\"Iter {k_iter}: Performing SVD with k={current_svd_k}...\")\n",
        "        svd_start_time = time.time()\n",
        "        try:\n",
        "            # Ensure k for svds is valid\n",
        "            k_svds = max(1, min(current_svd_k, M_movies_active - 1, N_users_active - 1))\n",
        "            if k_svds <= 0:\n",
        "                 logger.warning(f\"Iter {k_iter}: Matrix dimensions too small for SVD. Skipping.\")\n",
        "                 rank_k = 0; S_new = np.array([], dtype=np.float64)\n",
        "                 U_new = np.zeros((M_movies_active, 0), dtype=np.float64)\n",
        "                 Vt_new = np.zeros((0, N_users_active), dtype=np.float64) # Need Vt shape\n",
        "            else:\n",
        "                # Use scipy's svds which works with LinearOperator\n",
        "                U_new, S_new_raw, Vt_new = svds(Z_op, k=k_svds, which='LM', tol=1e-4, maxiter=100) # Adjust svds tol/maxiter if needed\n",
        "\n",
        "            # svds returns sorted singular values (largest first) - reverse order\n",
        "            S_new_raw = S_new_raw[::-1]\n",
        "            U_new = U_new[:, ::-1]\n",
        "            Vt_new = Vt_new[::-1, :]\n",
        "\n",
        "            S_new = soft_threshold(S_new_raw, lambda_reg) # Threshold\n",
        "            V_new = Vt_new.T # Transpose Vt to get V\n",
        "            rank_k = int(np.sum(S_new > 1e-10))\n",
        "\n",
        "            logger.debug(f\"Iter {k_iter}: SVD finished in {time.time() - svd_start_time:.2f}s. Rank after thresholding: {rank_k}\")\n",
        "\n",
        "            if rank_k == 0:\n",
        "                 logger.warning(f\"Iter {k_iter}: Rank became zero. Resetting.\")\n",
        "                 current_svd_k = 1 # Reset k for next SVD\n",
        "                 U = np.zeros((M_movies_active, 1), dtype=np.float64)\n",
        "                 S = np.zeros(1, dtype=np.float64)\n",
        "                 V = np.zeros((N_users_active, 1), dtype=np.float64)\n",
        "            else:\n",
        "                 U = U_new[:, :rank_k].copy()\n",
        "                 S = S_new[:rank_k].copy()\n",
        "                 V = V_new[:, :rank_k].copy()\n",
        "                 current_svd_k = min(rank_k + 5, CONVEX_RANK_K) # Increase k slightly for next iter, capped\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SVD failed during SoftImpute iter {k_iter}: {e}\", exc_info=True)\n",
        "            break\n",
        "\n",
        "        # Convergence Check\n",
        "        U_diff_norm = np.linalg.norm(U - U_old, 'fro'); S_diff_norm = np.linalg.norm(S - S_old, 'fro'); V_diff_norm = np.linalg.norm(V - V_old, 'fro')\n",
        "        U_norm = max(1.0, np.linalg.norm(U_old, 'fro')); S_norm = max(1.0, np.linalg.norm(S_old, 'fro')); V_norm = max(1.0, np.linalg.norm(V_old, 'fro'))\n",
        "        relative_diff = max(U_diff_norm / U_norm, S_diff_norm / S_norm, V_diff_norm / V_norm) if U_norm > 0 and S_norm > 0 and V_norm > 0 else np.inf\n",
        "        logger.debug(f\"Iter {k_iter}: Max Rel Factor Diff={relative_diff:.4e}, Rank={rank_k}\")\n",
        "\n",
        "        # Evaluate Metrics\n",
        "        eval_start_time = time.time()\n",
        "        try:\n",
        "            # Objective: 0.5 * ||P_Omega(X - R_orig)||_F^2 + lambda * ||X||_*\n",
        "            rows, cols = R_train_coo_orig.row, R_train_coo_orig.col\n",
        "            vals_orig = R_train_coo_orig.data\n",
        "            preds_at_omega_k = np.array([np.dot(U[r, :], S * V[c, :]) for r, c in zip(rows, cols)], dtype=np.float64)\n",
        "            loss_obs_k = 0.5 * np.sum((preds_at_omega_k - vals_orig)**2)\n",
        "            nuclear_norm_k = np.sum(S)\n",
        "            loss_k = loss_obs_k + lambda_reg * nuclear_norm_k\n",
        "\n",
        "            # RMSE: Predict original scale ratings (USV^T) and compare to true validation ratings\n",
        "            dot_prods_probe = np.array([np.dot(U[m, :], S * V[u, :]) for m, u in zip(probe_movies_mapped, probe_users_mapped)], dtype=np.float64)\n",
        "            preds_probe_clamped = np.clip(dot_prods_probe, 1.0, 5.0) # Clamp prediction\n",
        "            valid_true_mask_probe = ~np.isnan(ratings_val_true)\n",
        "            if np.any(valid_true_mask_probe):\n",
        "                 mse_probe = np.mean((preds_probe_clamped[valid_true_mask_probe] - ratings_val_true[valid_true_mask_probe])**2)\n",
        "                 rmse_k = np.sqrt(mse_probe) if mse_probe >= 0 else np.nan\n",
        "            else: rmse_k = np.nan\n",
        "\n",
        "        except Exception as e: logger.error(f\"Error during SoftImpute evaluation: {e}\"); loss_k, rmse_k, rank_k = np.nan, np.nan, rank_k if 'rank_k' in locals() else -1\n",
        "\n",
        "        eval_time = time.time() - eval_start_time\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time); hist_rank.append(rank_k)\n",
        "        U_old, S_old, V_old = U.copy(), S.copy(), V.copy() # Update for next convergence check\n",
        "\n",
        "        iter_time = time.time() - iter_start_time\n",
        "        logger.info(f\"Iter {k_iter:02d}: Loss={loss_k:.4e}, RMSE={rmse_k:.4f}, Rank={rank_k}, Rel Diff={relative_diff:.4e} (Eval: {eval_time:.2f}s, Total: {iter_time:.2f}s)\")\n",
        "\n",
        "        if relative_diff < tol: logger.info(f\"Soft-Impute converged at iteration {k_iter}\"); break\n",
        "\n",
        "    logger.info(\"Efficient Convex Soft-Impute Solver Finished.\")\n",
        "    return {'loss': hist_loss, 'rmse': hist_rmse, 'time': hist_time, 'rank': hist_rank, 'U': U, 'S': S, 'V': V}\n",
        "\n",
        "\n",
        "# --- Stochastic Solvers (SARAH, SPIDER) ---\n",
        "class RiemannianSARAH: # Adapted from long.txt\n",
        "    def __init__(self, R, P, g_i, g_batch, batch_size=100, m=1000, eta=1e-3, rng=None):\n",
        "        self.R, self.P, self.g_i, self.g_batch = R, P, g_i, g_batch\n",
        "        self.B, self.m, self.eta = batch_size, m, eta\n",
        "        self.rng = default_rng(rng) if rng is None else rng\n",
        "    def run(self, U0, n_steps, grad_args, active_idx, sampling_prob=None):\n",
        "        if active_idx is None or len(active_idx) == 0: return U0\n",
        "        rng = self.rng; U = U0.copy().astype(np.float32); v = np.zeros_like(U0, dtype=np.float32)\n",
        "        U_prev = U.copy().astype(np.float32); num_active = len(active_idx)\n",
        "        for t in range(n_steps):\n",
        "            if t % self.m == 0:\n",
        "                current_batch_size = min(self.B, num_active);\n",
        "                if current_batch_size == 0: continue\n",
        "                batch_indices = rng.choice(active_idx, size=current_batch_size, p=sampling_prob, replace=True)\n",
        "                try: v = self.g_batch(U, batch_indices, *grad_args).astype(np.float32)\n",
        "                except Exception as e: logger.error(f\"SARAH refresh grad error: {e}\"); v = np.zeros_like(U)\n",
        "                if not np.isfinite(v).all(): logger.warning(f\"SARAH non-finite refresh grad step {t}\"); v = np.zeros_like(U)\n",
        "            else:\n",
        "                if num_active == 0: continue\n",
        "                i_idx = rng.choice(active_idx, size=1, p=sampling_prob, replace=True)[0]; i = int(i_idx)\n",
        "                try:\n",
        "                    v_new = self.g_i(U, i, *grad_args).astype(np.float32)\n",
        "                    v_old = self.g_i(U_prev, i, *grad_args).astype(np.float32)\n",
        "                    if np.isfinite(v_new).all() and np.isfinite(v_old).all(): v += v_new - v_old\n",
        "                except Exception as e: logger.error(f\"SARAH single grad error user {i}: {e}\")\n",
        "            G_proj = self.P(U, v); step = (-self.eta * G_proj).astype(np.float32)\n",
        "            if should_stop_subproblem(G_proj, step): break\n",
        "            U_prev = U.copy(); U_next = self.R(U, step)\n",
        "            if not np.isfinite(U_next).all(): logger.warning(f\"SARAH non-finite U step {t+1}\"); U = U_prev; break\n",
        "            U = U_next\n",
        "        return U\n",
        "class RiemannianSPIDER: # Adapted from long.txt\n",
        "    def __init__(self, retraction, proj, grad_i, grad_batch, m=100, step=1e-3, rng=None):\n",
        "        self.R = retraction; self.P = proj; self.g_i = grad_i; self.g_batch = grad_batch\n",
        "        self.m = m; self.eta = step\n",
        "        self.rng = default_rng(rng) if rng is None else rng\n",
        "    def run(self, U0, n_steps, grad_args, active_idx, sampling_prob=None):\n",
        "        if active_idx is None or len(active_idx) == 0: return U0\n",
        "        rng = self.rng; U = U0.copy().astype(np.float32); v = np.zeros_like(U0, dtype=np.float32)\n",
        "        U_prev = U0.copy().astype(np.float32); num_active = len(active_idx)\n",
        "        for t in range(n_steps):\n",
        "            if t % self.m == 0:\n",
        "                current_batch_size = min(self.m, num_active); # Use m as batch size for refresh\n",
        "                if current_batch_size == 0: continue\n",
        "                batch_indices = rng.choice(active_idx, size=current_batch_size, p=sampling_prob, replace=True)\n",
        "                try: v = self.g_batch(U, batch_indices, *grad_args).astype(np.float32)\n",
        "                except Exception as e: logger.error(f\"SPIDER refresh grad error: {e}\"); v = np.zeros_like(U)\n",
        "                if not np.isfinite(v).all(): logger.warning(f\"SPIDER non-finite refresh grad step {t}\"); v = np.zeros_like(U)\n",
        "            else:\n",
        "                if num_active == 0: continue\n",
        "                i_idx = rng.choice(active_idx, size=1, p=sampling_prob, replace=True)[0]; i = int(i_idx)\n",
        "                try:\n",
        "                    grad_new = self.g_i(U, i, *grad_args).astype(np.float32)\n",
        "                    grad_old = self.g_i(U_prev, i, *grad_args).astype(np.float32)\n",
        "                    if np.isfinite(grad_new).all() and np.isfinite(grad_old).all(): v = v + grad_new - grad_old\n",
        "                except Exception as e: logger.error(f\"SPIDER single grad error user {i}: {e}\")\n",
        "            G_proj = self.P(U, v); step_vec = (-self.eta * G_proj).astype(np.float32)\n",
        "            if should_stop_subproblem(G_proj, step_vec): break\n",
        "            U_prev = U.copy(); U_next = self.R(U, step_vec)\n",
        "            if not np.isfinite(U_next).all(): logger.warning(f\"SPIDER non-finite U step {t+1}\"); U = U_prev; break\n",
        "            U = U_next\n",
        "        return U\n",
        "# --- RGD Solver ---\n",
        "\n",
        "def run_rgd_with_biases(\n",
        "    R_train_coo, global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true,\n",
        "    N_users_active, M_movies_active, rank_local, n_iters,\n",
        "    lam_sq, lam_bias, rng, init_scale=INIT_SCALE_NON_CONVEX,\n",
        "    lr_init=INIT_LR_RIEMANN, ls_beta=LS_BETA, ls_sigma=LS_SIGMA\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"Runs Riemannian Gradient Descent with biases.\"\"\"\n",
        "    logger.info(\"Starting RGD Solver with Biases...\")\n",
        "    U, W, user_bias, movie_bias = initialize_factors_and_biases(M_movies_active, N_users_active, rank_local, rng, init_scale)\n",
        "    hist_loss, hist_rmse, hist_time, hist_grad_norm = [], [], [], []\n",
        "    start_time = time.time(); lr_k = lr_init\n",
        "    loss_args_biased = (global_mean, R_train_coo.row, R_train_coo.col, R_train_coo.data, M_movies_active, N_users_active, rank_local, lam_sq, lam_bias)\n",
        "    eval_args_biased = (global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true)\n",
        "    try:\n",
        "        loss_k, rmse_k, gU_k, gW_k, gBu_k, gBi_k = record_initial_state_biased(U, W, user_bias, movie_bias, loss_args_biased, eval_args_biased)\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time)\n",
        "        gU_proj_k = ProjTangent(U, gU_k); hist_grad_norm.append(np.linalg.norm(gU_proj_k))\n",
        "    except Exception as e: logger.error(f\"RGD Init Error: {e}\"); return {'loss': [], 'rmse': [], 'time': [], 'grad_norm': []}\n",
        "\n",
        "    for k in range(n_iters):\n",
        "        iter_start_time = time.time()\n",
        "        gU_proj_k = ProjTangent(U, gU_k)\n",
        "        grad_norm_k = np.linalg.norm(gU_proj_k)\n",
        "        hist_grad_norm.append(grad_norm_k)\n",
        "\n",
        "        # --- FIX: Check Riemannian Gradient Norm ---\n",
        "        if grad_norm_k < 1e-6: logger.info(\"RGD Converged (grad norm)\"); break\n",
        "        # -------------------------------------------\n",
        "\n",
        "        ls_loss_args = (W, user_bias, movie_bias) + loss_args_biased\n",
        "        lr_step, U_next, loss_next = ArmijoLineSearchRiemannian(U, gU_k, ls_loss_args, loss_k, lr_k, ls_beta, ls_sigma)\n",
        "        if lr_step == 0.0: logger.warning(\"RGD Line search failed.\"); break\n",
        "\n",
        "        lr_fixed_other = 1e-4\n",
        "        W -= lr_fixed_other * gW_k; user_bias -= lr_fixed_other * gBu_k; movie_bias -= lr_fixed_other * gBi_k\n",
        "        U = U_next; loss_k = loss_next\n",
        "        lr_k = min(lr_step / np.sqrt(ls_beta), lr_init * 2)\n",
        "\n",
        "        _, gU_k, gW_k, gBu_k, gBi_k = loss_and_grad_serial_with_biases(U, W, user_bias, movie_bias, *loss_args_biased)\n",
        "        rmse_k = evaluate_rmse_with_biases(U, W, user_bias, movie_bias, *eval_args_biased)\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time)\n",
        "        iter_time = time.time() - iter_start_time\n",
        "        logger.info(f\"Iter {k+1:02d}: Loss={loss_k:.4e}, RMSE={rmse_k:.4f}, GradNorm={grad_norm_k:.2e}, LR={lr_step:.2e} (Time: {iter_time:.2f}s)\")\n",
        "\n",
        "    logger.info(\"RGD Solver Finished.\")\n",
        "\n",
        "# --- RAGD Solver ---\n",
        "\n",
        "#\n",
        "# --- RAGD Solver ---\n",
        "def run_ragd_with_biases(\n",
        "    R_train_coo, global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true,\n",
        "    N_users_active, M_movies_active, rank_local, n_iters,\n",
        "    lam_sq, lam_bias, rng, init_scale=INIT_SCALE_NON_CONVEX,\n",
        "    lr_init=INIT_LR_RIEMANN, ls_beta=LS_BETA, ls_sigma=LS_SIGMA,\n",
        "    gamma=RAGD_GAMMA, mu=RAGD_MU, beta_ragd=RAGD_BETA\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"Runs Riemannian Accelerated Gradient Descent with biases.\"\"\"\n",
        "    logger.info(\"Starting RAGD Solver with Biases...\")\n",
        "    U_k, W_k, user_bias_k, movie_bias_k = initialize_factors_and_biases(M_movies_active, N_users_active, rank_local, rng, init_scale)\n",
        "    nu_k = U_k.copy() # Momentum state\n",
        "    gamma_k = gamma\n",
        "    min_lambda_k = lr_init\n",
        "\n",
        "    hist_loss, hist_rmse, hist_time, hist_grad_norm = [], [], [], []\n",
        "    start_time = time.time()\n",
        "\n",
        "    loss_args_biased = (global_mean, R_train_coo.row, R_train_coo.col, R_train_coo.data, M_movies_active, N_users_active, rank_local, lam_sq, lam_bias)\n",
        "    eval_args_biased = (global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true)\n",
        "\n",
        "    try:\n",
        "        loss_k, rmse_k, gU_k, gW_k, gBu_k, gBi_k = record_initial_state_biased(U_k, W_k, user_bias_k, movie_bias_k, loss_args_biased, eval_args_biased)\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time)\n",
        "        gU_proj_k = ProjTangent(U_k, gU_k); hist_grad_norm.append(np.linalg.norm(gU_proj_k))\n",
        "    except Exception as e: logger.error(f\"RAGD Init Error: {e}\"); return {'loss': [], 'rmse': [], 'time': [], 'grad_norm': []}\n",
        "\n",
        "    def solve_alpha_eqn(current_min_lambda, gamma, mu):\n",
        "        a = 1.0; b = current_min_lambda * (gamma - mu); c = -current_min_lambda * gamma\n",
        "        delta = b**2 - 4*a*c\n",
        "        if delta < 0: return 0.0\n",
        "        alpha1 = (-b + np.sqrt(delta))/(2*a); alpha2 = (-b - np.sqrt(delta))/(2*a)\n",
        "        if 0 < alpha1 < 1: return alpha1\n",
        "        if 0 < alpha2 < 1: return alpha2\n",
        "        return 0.0\n",
        "\n",
        "    for k in range(n_iters):\n",
        "        iter_start_time = time.time()\n",
        "        logger.info(f\"--- Starting RAGD Iteration {k+1:02d} ---\")\n",
        "\n",
        "        alpha = solve_alpha_eqn(min_lambda_k, gamma_k, mu)\n",
        "        if alpha == 0.0: alpha = 1e-6 # Avoid division by zero / stagnation\n",
        "        gamma_bar = (1 - alpha) * gamma_k + alpha * mu\n",
        "        if gamma_bar == 0.0: gamma_bar = 1e-6\n",
        "\n",
        "        # Extrapolation step for y_t (only on U)\n",
        "        logmap_nu_theta = LogMapApprox(U_k, nu_k)\n",
        "        y_t = OrthRetraction(U_k, (alpha * gamma_k / gamma_bar) * logmap_nu_theta)\n",
        "\n",
        "        # Gradient at y_t (need W and biases at y_t? Assume they stay at k for simplicity)\n",
        "        loss_yt, gU_yt, gW_yt, gBu_yt, gBi_yt = loss_and_grad_serial_with_biases(\n",
        "            y_t, W_k, user_bias_k, movie_bias_k, *loss_args_biased\n",
        "        )\n",
        "\n",
        "        # Line search from y_t to find theta_{k+1} (U_{k+1})\n",
        "        ls_loss_args = (W_k, user_bias_k, movie_bias_k) + loss_args_biased\n",
        "        lr_step, U_kp1, loss_kp1 = ArmijoLineSearchRiemannian(\n",
        "            y_t, gU_yt, ls_loss_args, loss_yt, min_lambda_k, ls_beta, ls_sigma\n",
        "        )\n",
        "\n",
        "        if lr_step == 0.0: logger.warning(\"RAGD Line search failed.\"); break\n",
        "        min_lambda_k = lr_step # Update min LR found\n",
        "\n",
        "        # Update nu (momentum state)\n",
        "        logmap_nu_yt = LogMapApprox(y_t, nu_k)\n",
        "        grad_proj_yt = ProjTangent(y_t, gU_yt)\n",
        "        nu_update_vec = ((1 - alpha) * gamma_k / gamma_bar) * logmap_nu_yt - (alpha / gamma_bar) * grad_proj_yt\n",
        "        nu_kp1 = OrthRetraction(y_t, nu_update_vec)\n",
        "\n",
        "        # Update W and biases (simple gradient step with decayed LR for stability)\n",
        "        lr_fixed_other = 1e-4 * (0.9**k) # Use a small decaying LR\n",
        "        W_kp1 = W_k - lr_fixed_other * gW_k\n",
        "        user_bias_kp1 = user_bias_k - lr_fixed_other * gBu_k\n",
        "        movie_bias_kp1 = movie_bias_k - lr_fixed_other * gBi_k\n",
        "\n",
        "        # Update state\n",
        "        U_k, W_k, user_bias_k, movie_bias_k = U_kp1, W_kp1, user_bias_kp1, movie_bias_kp1\n",
        "        nu_k = nu_kp1\n",
        "        gamma_k = gamma_bar / (1 + beta_ragd) # Update gamma\n",
        "        loss_k = loss_kp1\n",
        "\n",
        "        # Evaluate and record\n",
        "        rmse_k = evaluate_rmse_with_biases(U_k, W_k, user_bias_k, movie_bias_k, *eval_args_biased)\n",
        "        # Recompute gradient at the final point U_k for norm calculation\n",
        "        _, gU_k_final, gW_k, gBu_k, gBi_k = loss_and_grad_serial_with_biases(U_k, W_k, user_bias_k, movie_bias_k, *loss_args_biased)\n",
        "        gU_proj_k = ProjTangent(U_k, gU_k_final)\n",
        "        grad_norm_k = np.linalg.norm(gU_proj_k)\n",
        "\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time)\n",
        "        hist_grad_norm.append(grad_norm_k)\n",
        "\n",
        "        iter_time = time.time() - iter_start_time\n",
        "        logger.info(f\"Iter {k+1:02d}: Loss={loss_k:.4e}, RMSE={rmse_k:.4f}, GradNorm={grad_norm_k:.2e}, LR={lr_step:.2e} (Time: {iter_time:.2f}s)\")\n",
        "\n",
        "        if grad_norm_k < 1e-6: logger.info(\"RAGD Converged (grad norm)\"); break\n",
        "\n",
        "    logger.info(\"RAGD Solver Finished.\")\n",
        "    return {'loss': hist_loss, 'rmse': hist_rmse, 'time': hist_time, 'grad_norm': hist_grad_norm, 'U': U_k, 'W': W_k, 'bu': user_bias_k, 'bi': movie_bias_k}\n",
        "\n",
        "# --- Catalyst Solver ---\n",
        "# --- Catalyst Solver (Modified for Stochastic Inner Solvers) ---\n",
        "def run_catalyst_stochastic( # Renamed from run_catalyst_phi2_with_biases\n",
        "    R_train_coo, global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true,\n",
        "    N_users_active, M_movies_active, rank_local, n_iters,\n",
        "    lam_sq, lam_bias, rng, init_scale=INIT_SCALE_NON_CONVEX,\n",
        "    lr_init=INIT_LR_RIEMANN, ls_beta=LS_BETA, ls_sigma=LS_SIGMA,\n",
        "    kappa_0=KAPPA_0, kappa_cvx=KAPPA_CVX, inner_T_epochs=CATALYST_INNER_T_EPOCHS,\n",
        "    inner_S_epochs_base=CATALYST_INNER_S_EPOCHS_BASE,\n",
        "    max_kappa_doublings=MAX_KAPPA_DOUBLINGS,\n",
        "    inner_solver_type=INNER_SOLVER, # NEW: Specify inner solver\n",
        "    inner_solver_lr = RSVRG_LR, # NEW: LR for stochastic inner solver\n",
        "    inner_solver_bs = RSVRG_BATCH_SIZE # NEW: Batch size for stochastic inner solver\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"Runs Catalyst-Phi2 using a specified stochastic Riemannian solver.\"\"\"\n",
        "    solver_name = inner_solver_type.upper()\n",
        "    logger.info(f\"Starting Catalyst-Phi2 + {solver_name} Solver with Biases...\")\n",
        "    theta_k, W_k, user_bias_k, movie_bias_k = initialize_factors_and_biases(M_movies_active, N_users_active, rank_local, rng, init_scale)\n",
        "    theta_km1 = theta_k.copy(); tilde_theta_km1 = theta_k.copy()\n",
        "    alpha_k = 1.0; kappa_k = kappa_0\n",
        "    hist_loss, hist_rmse, hist_time, hist_grad_norm = [], [], [], []\n",
        "    phi1_grad_hist, phi1_dist_hist = [], [] # Rank 0 diagnostics\n",
        "    start_time = time.time()\n",
        "    loss_args_biased = (global_mean, R_train_coo.row, R_train_coo.col, R_train_coo.data, M_movies_active, N_users_active, rank_local, lam_sq, lam_bias)\n",
        "    eval_args_biased = (global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true)\n",
        "    grad_args_stoch = (N_users_active, M_movies_active, loss_args_biased) # Args for stochastic grad funcs\n",
        "    n_data = R_train_coo.nnz # Use number of ratings for epoch length calculation? Or users? Use users.\n",
        "    n_active_users = N_users_active\n",
        "    epoch_len_batches = max(1, n_active_users // inner_solver_bs) if n_active_users > 0 else 1\n",
        "\n",
        "    try:\n",
        "        loss_k, rmse_k, gU_k, gW_k, gBu_k, gBi_k = record_initial_state_biased(theta_k, W_k, user_bias_k, movie_bias_k, loss_args_biased, eval_args_biased)\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time)\n",
        "        gU_proj_k = ProjTangent(theta_k, gU_k); hist_grad_norm.append(np.linalg.norm(gU_proj_k))\n",
        "    except Exception as e: logger.error(f\"Catalyst-{solver_name} Init Error: {e}\"); return {'loss': [], 'rmse': [], 'time': [], 'grad_norm': []}\n",
        "\n",
        "    # Instantiate selected inner solver (consistent across ranks)\n",
        "    inner_solver_instance = None\n",
        "    refresh_period_m = max(1, epoch_len_batches // 2) # Example refresh period\n",
        "    solver_args_inner = {\n",
        "        'R': R_fn, 'P': ProjTangent, 'eta': inner_solver_lr,\n",
        "        'g_i': stochastic_gradient_single_user, 'g_batch': stochastic_gradient_batch,\n",
        "        'g_batch': stochastic_gradient_batch,     # now resolved 5/4/2025\n",
        "        'rng': default_rng(SEED + 1 + RANK_MPI) # Ensure different RNG streams per rank\n",
        "    }\n",
        "    if inner_solver_type == \"sarah\": InnerSolverClass = RiemannianSARAH; solver_args_inner.update({'batch_size': inner_solver_bs, 'm': refresh_period_m})\n",
        "    elif inner_solver_type == \"spider\": InnerSolverClass = RiemannianSPIDER; solver_args_inner.update({'m': refresh_period_m})\n",
        "    elif inner_solver_type == \"svrg\": InnerSolverClass = None # SVRG logic remains embedded\n",
        "    else: raise ValueError(f\"Unknown INNER_SOLVER: {inner_solver_type}\")\n",
        "    if InnerSolverClass: inner_solver_instance = InnerSolverClass(**solver_args_inner)\n",
        "\n",
        "    for k in range(1, n_iters + 1):\n",
        "        iter_start_time = time.time()\n",
        "        logger.info(f\"--- Starting Catalyst-{solver_name} Iteration {k:02d} ---\")\n",
        "        kappa_step1 = kappa_k; doubling_count = 0\n",
        "        inner_T_steps_budget = epoch_len_batches * inner_T_epochs # Steps budget\n",
        "\n",
        "        logger.debug(f\"Iter {k}: Running Phi1 (kappa adaptation)...\")\n",
        "        while True:\n",
        "            prox_center = theta_km1.copy()\n",
        "            # --- Run Inner Solver for Step 1 ---\n",
        "            U_inner1 = None\n",
        "            if InnerSolverClass:\n",
        "                 try:\n",
        "                      logger.warning(f\"Running inner {solver_name} on f, not h_kappa in Phi1.\")\n",
        "                      solver_args_run = (grad_args_stoch, unique_users_train, sampling_prob) # Pass active user IDs\n",
        "                      U_inner1 = inner_solver_instance.run(prox_center, inner_T_steps_budget, *solver_args_run)\n",
        "                 except Exception as e_inner: logger.error(f\"Inner {solver_name} (Step 1) failed: {e_inner}\"); U_inner1 = prox_center\n",
        "            else: # Embedded SVRG for Step 1 subproblem\n",
        "                U_snapshot = prox_center.copy()\n",
        "                G_full_snapshot = np.zeros_like(U_snapshot) # Calculate full gradient estimate\n",
        "                if n_active_users > 0:\n",
        "                    num_batches_for_full_grad = max(1, math.ceil(n_active_users / inner_solver_bs / 5))\n",
        "                    count_full = 0\n",
        "                    for _ in range(num_batches_for_full_grad):\n",
        "                        current_batch_size = min(inner_solver_bs, n_active_users)\n",
        "                        if current_batch_size == 0: continue\n",
        "                        batch_ids_full = GLOBAL_RNG.choice(unique_users_train, size=current_batch_size, p=sampling_prob, replace=True)\n",
        "                        try: G_batch = stochastic_gradient_batch(U_snapshot, batch_ids_full, *grad_args_stoch);\n",
        "                        except Exception: continue\n",
        "                        if np.isfinite(G_batch).all(): G_full_snapshot += G_batch; count_full += 1\n",
        "                    if count_full > 0: G_full_snapshot /= count_full\n",
        "                U_inner1_svrg = U_snapshot.copy();\n",
        "                for i_t in range(inner_T_steps_budget):\n",
        "                    current_batch_size = min(inner_solver_bs, n_active_users)\n",
        "                    if current_batch_size == 0: break\n",
        "                    batch_ids = GLOBAL_RNG.choice(unique_users_train, size=current_batch_size, p=sampling_prob, replace=True)\n",
        "                    try: g_curr = stochastic_gradient_batch(U_inner1_svrg, batch_ids, *grad_args_stoch); g_ref  = stochastic_gradient_batch(U_snapshot, batch_ids, *grad_args_stoch)\n",
        "                    except Exception: g_curr = np.zeros_like(U_inner1_svrg); g_ref = np.zeros_like(U_inner1_svrg)\n",
        "                    if not (np.isfinite(g_curr).all() and np.isfinite(g_ref).all()): continue\n",
        "                    G_vr_f = g_curr - g_ref + G_full_snapshot\n",
        "                    if REG_DISTANCE == \"euclid\": G_prox_term = kappa_step1 * (U_inner1_svrg - prox_center);\n",
        "                    else: G_prox_term = - kappa_step1 * LogMapApprox(U_inner1_svrg, prox_center)\n",
        "                    subprob_G_vr_euclidean = G_vr_f + G_prox_term\n",
        "                    G_proj_vr = ProjTangent(U_inner1_svrg, subprob_G_vr_euclidean)\n",
        "                    step_vec = (-inner_solver_lr * G_proj_vr).astype(np.float32)\n",
        "                    if should_stop_subproblem(G_proj_vr, step_vec): break\n",
        "                    U_next_svrg = R_fn(U_inner1_svrg, step_vec)\n",
        "                    if not np.isfinite(U_next_svrg).all(): break\n",
        "                    U_inner1_svrg = U_next_svrg\n",
        "                U_inner1 = U_inner1_svrg\n",
        "\n",
        "            # --- Check conditions after inner solve ---\n",
        "            theta_bar_k_T = U_inner1;\n",
        "            try: loss_bar_k_T, G_bar_k_T = loss_and_grad_corrected(theta_bar_k_T, *loss_args);\n",
        "            except Exception as e: logger.error(f\"Error evaluating bar_theta: {e}\"); loss_bar_k_T = np.inf\n",
        "            if not np.isfinite(loss_bar_k_T): kappa_step1 *= 2; doubling_count += 1; continue\n",
        "            conditions_met = False; phi1_grad_norm = np.nan; d_R_approx = np.nan\n",
        "            if RANK_MPI == 0: # Only rank 0 checks conditions\n",
        "                d_R_approx = np.linalg.norm(LogMapApprox(theta_km1, theta_bar_k_T));\n",
        "                h_k_bar = loss_bar_k_T + 0.5 * kappa_step1 * d_R_approx**2;\n",
        "                loss_km1 = hist_loss[-1] if hist_loss else np.inf\n",
        "                descent_cond_met = (h_k_bar <= loss_km1 + 1e-9 * (1 + abs(loss_km1)))\n",
        "                if REG_DISTANCE == \"euclid\": subprob_grad_bar_k = G_bar_k_T + kappa_step1 * (theta_bar_k_T - theta_km1);\n",
        "                else: subprob_grad_bar_k = G_bar_k_T - kappa_step1 * LogMapApprox(theta_bar_k_T, theta_km1)\n",
        "                proj_grad_h = ProjTangent(theta_bar_k_T, subprob_grad_bar_k)\n",
        "                phi1_grad_norm = np.linalg.norm(proj_grad_h)\n",
        "                stationarity_rhs = kappa_step1 * d_R_approx\n",
        "                stat_cond_met = phi1_grad_norm <= stationarity_rhs + 1e-9 * (1 + stationarity_rhs)\n",
        "                if descent_cond_met and stat_cond_met:\n",
        "                    print(f\"      Alg phi_1 Conditions MET kappa={kappa_step1:.1e}\")\n",
        "                    phi1_grad_hist.append(phi1_grad_norm); phi1_dist_hist.append(d_R_approx)\n",
        "                    kappa_k_next = update_kappa_adaptive(kappa_step1, phi1_grad_hist, phi1_dist_hist, theta_bar_k_T)\n",
        "                    if abs(kappa_k_next - kappa_step1) > 1e-9: print(f\"      Adapting kappa next iter: {kappa_step1:.1e} -> {kappa_k_next:.1e}\")\n",
        "                    kappa_k = kappa_k_next\n",
        "                    conditions_met = True\n",
        "                else: print(f\"      Alg phi_1 Conditions NOT MET (Desc:{descent_cond_met}, Stat:{stat_cond_met}) kappa={kappa_step1:.1e}. Doubling.\")\n",
        "            if COMM and SIZE_MPI > 1: conditions_met = COMM.bcast(conditions_met, root=0); kappa_k = COMM.bcast(kappa_k, root=0) if conditions_met else kappa_k\n",
        "            if conditions_met: break\n",
        "            else:\n",
        "                kappa_step1 *= 2; doubling_count += 1;\n",
        "                if doubling_count >= MAX_KAPPA_DOUBLINGS: logger.warning(\"Phi1 max kappa doublings reached.\"); break\n",
        "        if doubling_count >= MAX_KAPPA_DOUBLINGS: logger.error(f\"Catalyst Iter {k}: Phi1 failed. Stopping.\"); break\n",
        "        bar_theta_k = theta_bar_k_T; loss_bar_k = loss_bar_k_T; G_bar_k = G_bar_k_T; kappa_k = kappa_step1\n",
        "        logger.debug(f\"Iter {k}: Phi1 finished. Final kappa={kappa_k:.2e}\")\n",
        "\n",
        "        # === Step 2: Extrapolation ===\n",
        "        if k == 1: V_extrap_approx = np.zeros_like(theta_km1)\n",
        "        else: V_extrap_approx = LogMapApprox(theta_km1, tilde_theta_km1)\n",
        "        vartheta_k = R_fn(theta_km1, alpha_k * V_extrap_approx);\n",
        "        if not np.isfinite(vartheta_k).all(): logger.error(f\"Step 2 non-finite iter {k}. Stopping.\"); break\n",
        "\n",
        "        # === Step 3: Accelerated Step (using chosen solver) ===\n",
        "        logger.debug(f\"Iter {k}: Running Phi2 (accelerated step)...\")\n",
        "        prox_center_S = vartheta_k.copy()\n",
        "        S_k_epochs = math.ceil(inner_S_epochs_base * math.log(k + 1))\n",
        "        max_inner_iter_2 = S_k_epochs * epoch_len_batches\n",
        "\n",
        "        theta_tilde_k = None\n",
        "        if InnerSolverClass:\n",
        "             try:\n",
        "                  logger.warning(f\"Running inner {solver_name} on f, not h_kappa_cvx in Phi2.\")\n",
        "                  solver_args_run_S = (grad_args_stoch, unique_users_train, sampling_prob)\n",
        "                  theta_tilde_k = inner_solver_instance.run(prox_center_S, max_inner_iter_2, *solver_args_run_S)\n",
        "             except Exception as e_inner_S: logger.error(f\"Inner {solver_name} (Step 3) failed: {e_inner_S}\"); theta_tilde_k = prox_center_S\n",
        "        else: # Embedded SVRG for Step 3 subproblem\n",
        "            U_snapshot_S = prox_center_S\n",
        "            G_full_snapshot_S = np.zeros_like(U_snapshot_S) # Calculate full gradient estimate\n",
        "            if n_active_users > 0:\n",
        "                 num_batches_for_full_grad_S = max(1, math.ceil(n_active_users / inner_solver_bs / 5))\n",
        "                 count_S_full = 0\n",
        "                 for _ in range(num_batches_for_full_grad_S):\n",
        "                      current_batch_size_S = min(inner_solver_bs, n_active_users)\n",
        "                      if current_batch_size_S == 0: continue\n",
        "                      batch_ids_full_S = GLOBAL_RNG.choice(unique_users_train, size=current_batch_size_S, p=sampling_prob, replace=True)\n",
        "                      try: G_batch_S = stochastic_gradient_batch(U_snapshot_S, batch_ids_full_S, *grad_args_stoch);\n",
        "                      except Exception: continue\n",
        "                      if np.isfinite(G_batch_S).all(): G_full_snapshot_S += G_batch_S; count_S_full += 1\n",
        "                 if count_S_full > 0: G_full_snapshot_S /= count_S_full\n",
        "            U_inner2_svrg = U_snapshot_S.copy();\n",
        "            for i_s in range(max_inner_iter_2):\n",
        "                 current_batch_size_S = min(inner_solver_bs, n_active_users)\n",
        "                 if current_batch_size_S == 0: break\n",
        "                 batch_ids_S = GLOBAL_RNG.choice(unique_users_train, size=current_batch_size_S, p=sampling_prob, replace=True)\n",
        "                 try: g_curr_S = stochastic_gradient_batch(U_inner2_svrg, batch_ids_S, *grad_args_stoch); g_ref_S  = stochastic_gradient_batch(U_snapshot_S, batch_ids_S, *grad_args_stoch)\n",
        "                 except Exception: g_curr_S = np.zeros_like(U_inner2_svrg); g_ref_S = np.zeros_like(U_inner2_svrg)\n",
        "                 if not (np.isfinite(g_curr_S).all() and np.isfinite(g_ref_S).all()): continue\n",
        "                 G_vr_f_S = g_curr_S - g_ref_S + G_full_snapshot_S\n",
        "                 if REG_DISTANCE == \"euclid\": G_prox_term_S = KAPPA_CVX * (U_inner2_svrg - prox_center_S);\n",
        "                 else: G_prox_term_S = - KAPPA_CVX * LogMapApprox(U_inner2_svrg, prox_center_S)\n",
        "                 subprob_G_vr_euclidean_S = G_vr_f_S + G_prox_term_S\n",
        "                 G_proj_vr_S = ProjTangent(U_inner2_svrg, subprob_G_vr_euclidean_S)\n",
        "                 step_vec_S = (-inner_solver_lr * G_proj_vr_S).astype(np.float32)\n",
        "                 if should_stop_subproblem(G_proj_vr_S, step_vec_S): break\n",
        "                 U_next_S = R_fn(U_inner2_svrg, step_vec_S)\n",
        "                 if not np.isfinite(U_next_S).all(): break\n",
        "                 U_inner2_svrg = U_next_S\n",
        "            theta_tilde_k = U_inner2_svrg\n",
        "\n",
        "        try: loss_tilde_k, G_tilde_k = loss_and_grad_corrected(theta_tilde_k, *loss_args);\n",
        "        except Exception as e: logger.error(f\"Error evaluating tilde_theta: {e}\"); loss_tilde_k = np.inf\n",
        "        if not (np.isfinite(loss_tilde_k) and np.isfinite(G_tilde_k).all()): logger.error(f\"Step 3 ({solver_name}) failed iter {k}. Stopping.\"); break\n",
        "\n",
        "        # === Step 4, 5, 6 (Consistent) ===\n",
        "        if loss_bar_k <= loss_tilde_k: theta_kp1, loss_kp1, G_kp1, selected = theta_bar_k, loss_bar_k, G_bar_k, \"bar\"\n",
        "        else: theta_kp1, loss_kp1, G_kp1, selected = theta_tilde_k, loss_tilde_k, G_tilde_k, \"tilde\"\n",
        "        V_update_approx = LogMapApprox(theta_km1, theta_tilde_k);\n",
        "        tilde_theta_k_next = R_fn(theta_km1, (1.0 / alpha_k) * V_update_approx);\n",
        "        if not np.isfinite(tilde_theta_k_next).all(): logger.error(f\"Step 5 non-finite iter {k}. Stopping.\"); break\n",
        "        alpha_kp1 = (math.sqrt(alpha_k**4 + 4 * alpha_k**2) - alpha_k**2) / 2.0\n",
        "\n",
        "        # --- Update state for next iteration ---\n",
        "        theta_km1 = theta_kp1.copy(); tilde_theta_km1 = tilde_theta_k_next.copy()\n",
        "        alpha_k = alpha_kp1; loss_k = loss_kp1\n",
        "        lr_fixed_other = 1e-4 * (0.9**k)\n",
        "        _, _, gW_kp1, gBu_kp1, gBi_kp1 = loss_and_grad_serial_with_biases(theta_kp1, W_k, user_bias_k, movie_bias_k, *loss_args_biased)\n",
        "        W_k -= lr_fixed_other * gW_kp1; user_bias_k -= lr_fixed_other * gBu_kp1; movie_bias_k -= lr_fixed_other * gBi_kp1\n",
        "\n",
        "        # --- Record History ---\n",
        "        rmse_k = evaluate_rmse_with_biases(theta_kp1, W_k, user_bias_k, movie_bias_k, *eval_args_biased)\n",
        "        gU_proj_k = ProjTangent(theta_kp1, G_kp1); grad_norm_k = np.linalg.norm(gU_proj_k)\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time); hist_grad_norm.append(grad_norm_k)\n",
        "        iter_time = time.time() - iter_start_time\n",
        "        logger.info(f\"Iter {k:02d}: Loss={loss_k:.4e}, RMSE={rmse_k:.4f}, GradNorm={grad_norm_k:.2e}, Kappa={kappa_k:.2e} (Time: {iter_time:.2f}s)\")\n",
        "        if grad_norm_k < 1e-6: logger.info(f\"Catalyst-{solver_name} Converged (grad norm)\"); break\n",
        "\n",
        "    logger.info(f\"Catalyst-{solver_name} Solver Finished.\")\n",
        "    if k == n_iters: # Append final grad norm if loop finished normally\n",
        "         _, gU_k_final, _, _, _ = loss_and_grad_serial_with_biases(theta_k, W_k, user_bias_k, movie_bias_k, *loss_args_biased)\n",
        "         gU_proj_k = ProjTangent(theta_k, gU_k_final); hist_grad_norm.append(np.linalg.norm(gU_proj_k))\n",
        "    return {'loss': hist_loss, 'rmse': hist_rmse, 'time': hist_time, 'grad_norm': hist_grad_norm, 'U': theta_k, 'W': W_k, 'bu': user_bias_k, 'bi': movie_bias_k}\n",
        "\n",
        "\n",
        "# --- DANE Solver ---\n",
        "\n",
        "# --- DANE Solver ---\n",
        "def run_dane_with_biases(\n",
        "    R_train_coo, global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true,\n",
        "    N_users_active, M_movies_active, rank_local, n_iters,\n",
        "    lam_sq, lam_bias, rng, init_scale=INIT_SCALE_NON_CONVEX,\n",
        "    lr_init=INIT_LR_RIEMANN, ls_beta=LS_BETA, ls_sigma=LS_SIGMA,\n",
        "    kappa=DANE_KAPPA\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"Runs DANE adaptation with biases.\"\"\"\n",
        "    logger.info(\"Starting DANE Solver with Biases...\")\n",
        "    theta_k, W_k, user_bias_k, movie_bias_k = initialize_factors_and_biases(M_movies_active, N_users_active, rank_local, rng, init_scale)\n",
        "    theta_km1 = theta_k.copy()\n",
        "\n",
        "    hist_loss, hist_rmse, hist_time, hist_grad_norm = [], [], [], []\n",
        "    start_time = time.time()\n",
        "    lr_k = lr_init\n",
        "\n",
        "    loss_args_biased = (global_mean, R_train_coo.row, R_train_coo.col, R_train_coo.data, M_movies_active, N_users_active, rank_local, lam_sq, lam_bias)\n",
        "    eval_args_biased = (global_mean, probe_users_mapped, probe_movies_mapped, probe_ratings_true)\n",
        "\n",
        "    try:\n",
        "        loss_k, rmse_k, gU_k, gW_k, gBu_k, gBi_k = record_initial_state_biased(theta_k, W_k, user_bias_k, movie_bias_k, loss_args_biased, eval_args_biased)\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time)\n",
        "        gU_proj_k = ProjTangent(theta_k, gU_k); hist_grad_norm.append(np.linalg.norm(gU_proj_k))\n",
        "    except Exception as e: logger.error(f\"DANE Init Error: {e}\"); return {'loss': [], 'rmse': [], 'time': [], 'grad_norm': []}\n",
        "\n",
        "    for k in range(n_iters):\n",
        "        iter_start_time = time.time()\n",
        "        logger.info(f\"--- Starting DANE Iteration {k+1:02d} ---\")\n",
        "\n",
        "        if k == 0:\n",
        "            grad_combined = gU_k # Use initial gradient for first step\n",
        "        else:\n",
        "            reg_grad = RegularizeGradChordalApprox(theta_k, theta_km1, kappa)\n",
        "            grad_combined = gU_k + reg_grad # gU_k is from end of previous iteration\n",
        "\n",
        "        gU_proj_k = ProjTangent(theta_k, grad_combined)\n",
        "        grad_norm_k = np.linalg.norm(gU_proj_k)\n",
        "        hist_grad_norm.append(grad_norm_k) # Log norm before step\n",
        "\n",
        "        if grad_norm_k < 1e-6: logger.info(\"DANE Converged (grad norm)\"); break\n",
        "\n",
        "        # Line search on U update using combined gradient\n",
        "        ls_loss_args = (W_k, user_bias_k, movie_bias_k) + loss_args_biased\n",
        "        lr_step, U_kp1, loss_kp1 = ArmijoLineSearchRiemannian(\n",
        "            theta_k, grad_combined, ls_loss_args, loss_k, lr_k, ls_beta, ls_sigma\n",
        "        )\n",
        "\n",
        "        if lr_step == 0.0: logger.warning(\"DANE Line search failed.\"); break\n",
        "\n",
        "        # Update W and biases (simple gradient step with decayed LR?)\n",
        "        lr_fixed_other = 1e-4 * (0.9**k)\n",
        "        W_kp1 = W_k - lr_fixed_other * gW_k\n",
        "        user_bias_kp1 = user_bias_k - lr_fixed_other * gBu_k\n",
        "        movie_bias_kp1 = movie_bias_k - lr_fixed_other * gBi_k\n",
        "\n",
        "        # Update state\n",
        "        theta_km1 = theta_k.copy() # Store previous U\n",
        "        theta_k = U_kp1\n",
        "        W_k, user_bias_k, movie_bias_k = W_kp1, user_bias_kp1, movie_bias_kp1\n",
        "        loss_k = loss_kp1\n",
        "        lr_k = min(lr_step / np.sqrt(ls_beta), lr_init * 2) # Update LR for next search\n",
        "\n",
        "        # Recompute gradients at new point for next iteration\n",
        "        _, gU_k, gW_k, gBu_k, gBi_k = loss_and_grad_serial_with_biases(theta_k, W_k, user_bias_k, movie_bias_k, *loss_args_biased)\n",
        "        rmse_k = evaluate_rmse_with_biases(theta_k, W_k, user_bias_k, movie_bias_k, *eval_args_biased)\n",
        "\n",
        "        hist_loss.append(loss_k); hist_rmse.append(rmse_k); hist_time.append(time.time() - start_time)\n",
        "\n",
        "        iter_time = time.time() - iter_start_time\n",
        "        logger.info(f\"Iter {k+1:02d}: Loss={loss_k:.4e}, RMSE={rmse_k:.4f}, GradNorm={grad_norm_k:.2e}, LR={lr_step:.2e} (Time: {iter_time:.2f}s)\")\n",
        "\n",
        "    logger.info(\"DANE Solver Finished.\")\n",
        "    return {'loss': hist_loss, 'rmse': hist_rmse, 'time': hist_time, 'grad_norm': hist_grad_norm, 'U': theta_k, 'W': W_k, 'bu': user_bias_k, 'bi': movie_bias_k}\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================ #\n",
        "# CELL 6: Convex Model Solver (Efficient Soft-Impute) - Renumbered\n",
        "# ============================================================================ #\n",
        "\"\"\"\n",
        "Soft‑Impute implementation (Mazumder et al., 2010)\n",
        "=================================================\n",
        "• Works with **NumPy/SciPy** on CPU and **CuPy** on GPU – the backend is\n",
        "  detected automatically.\n",
        "• Accepts\n",
        "    – `X_incomplete` as a dense `numpy.ndarray` / `cupy.ndarray` *or*\n",
        "      a sparse `scipy.sparse` / `cupyx.scipy.sparse` matrix whose\n",
        "      *missing* entries are encoded as **NaN**.\n",
        "• Returns either a fully‑filled dense array *or* the `(U,S,V)` factors.\n",
        "\n",
        "This is intentionally self‑contained – you can drop the file into any\n",
        "project (pure Python, no extra deps beyond SciPy/CuPy).\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import warnings\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import numpy as _np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "try:\n",
        "    import cupy as _cp\n",
        "    import cupyx.scipy.sparse as _cpx_sparse\n",
        "    _HAS_CUPY = True\n",
        "except ImportError:  # GPU unavailable\n",
        "    _cp = None  # type: ignore\n",
        "    _HAS_CUPY = False\n",
        "\n",
        "import scipy.sparse as _sp\n",
        "from scipy.sparse.linalg import svds as _svds  # CPU truncated SVD\n",
        "\n",
        "Array = Union[_np.ndarray, \"_cp.ndarray\"]  # forward reference for CuPy\n",
        "Sparse = Union[_sp.spmatrix, \"_cpx_sparse.spmatrix\"]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# helpers\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def _to_backend(x: Array | Sparse, use_gpu: bool):\n",
        "    \"\"\"Move *dense* or *sparse* array to the requested backend.\"\"\"\n",
        "    if use_gpu and not _HAS_CUPY:\n",
        "        raise RuntimeError(\"CuPy requested but not installed.\")\n",
        "\n",
        "    if use_gpu:\n",
        "        if _HAS_CUPY and isinstance(x, _cp.ndarray | _cpx_sparse.spmatrix):\n",
        "            return x  # already on GPU\n",
        "        return _cp.asarray(x) if not _sp.issparse(x) else _cpx_sparse.csr_matrix(x)\n",
        "    # -> CPU\n",
        "    if isinstance(x, _np.ndarray | _sp.spmatrix):\n",
        "        return x\n",
        "    return _cp.asnumpy(x) if not _sp.issparse(x) else _sp.csr_matrix(x.get())\n",
        "\n",
        "\n",
        "def _soft_threshold(s: Array, lam: float):\n",
        "    return _np.maximum(s - lam, 0.0)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# main class\n",
        "# ---------------------------------------------------------------------------\n",
        "class SoftImpute:\n",
        "    \"\"\"Matrix completion via nuclear‑norm minimisation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lam : float\n",
        "        Regularisation (shrinkage) parameter `λ`.\n",
        "    max_rank : int | None, optional\n",
        "        Maximum rank of the factorisation.  Defaults to `min(m, n)`.\n",
        "    max_iters : int, optional\n",
        "        Maximum number of iterations (default 100).\n",
        "    tol : float, optional\n",
        "        Stop when relative change in Frobenius norm < `tol` (default 1e‑4).\n",
        "    init_fill_method : {\"zero\", \"mean\"}\n",
        "        How to fill missing values in the first iteration.\n",
        "    use_gpu : bool, optional\n",
        "        *True* – try CuPy; *False* – force CPU; *None* – auto‑detect.\n",
        "    random_state : int | None\n",
        "        RNG seed for reproducible power‑iteration initialisation.\n",
        "    return_factors : bool, default False\n",
        "        If *True* return `(U, S, V)` instead of the filled matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        lam: float = 5.0,\n",
        "        *,\n",
        "        max_rank: Optional[int] = None,\n",
        "        max_iters: int = 100,\n",
        "        tol: float = 1e-4,\n",
        "        init_fill_method: str = \"zero\",\n",
        "        use_gpu: Optional[bool] = None,\n",
        "        random_state: Optional[int] = None,\n",
        "        return_factors: bool = False,\n",
        "    ) -> None:\n",
        "        self.lam = float(lam)\n",
        "        self.max_rank = max_rank\n",
        "        self.max_iters = int(max_iters)\n",
        "        self.tol = float(tol)\n",
        "        if init_fill_method not in {\"zero\", \"mean\"}:\n",
        "            raise ValueError(\"init_fill_method must be 'zero' or 'mean'\")\n",
        "        self.init_fill_method = init_fill_method\n",
        "        self.use_gpu = (_HAS_CUPY if use_gpu is None else bool(use_gpu))\n",
        "        self.rng = default_rng(random_state)\n",
        "        self.return_factors = return_factors\n",
        "\n",
        "        # will be initialised in `fit_transform`\n",
        "        self.U_: Optional[Array] = None\n",
        "        self.S_: Optional[Array] = None\n",
        "        self.V_: Optional[Array] = None\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    def fit_transform(self, X: Array | Sparse) -> Array | Tuple[Array, Array, Array]:\n",
        "        \"\"\"Run Soft‑Impute and return the completed matrix or the factors.\"\"\"\n",
        "\n",
        "        # move data to desired backend\n",
        "        X = _to_backend(X, self.use_gpu)\n",
        "        xp = _cp if (self.use_gpu) else _np\n",
        "        spmod = _cpx_sparse if (self.use_gpu) else _sp\n",
        "\n",
        "        # sparse → dense with NaNs where missing ------------------------------------------------\n",
        "        if spmod.issparse(X):\n",
        "            X = X.tocsr()\n",
        "            m, n = X.shape\n",
        "            dense = xp.full((m, n), xp.nan, dtype=xp.float32)\n",
        "            rows, cols = X.nonzero()\n",
        "            dense[rows, cols] = X.data.astype(xp.float32)\n",
        "            X = dense\n",
        "        else:\n",
        "            X = X.astype(xp.float32)\n",
        "\n",
        "        nan_mask = xp.isnan(X)\n",
        "        m, n = X.shape\n",
        "        max_rank = self.max_rank or min(m, n)\n",
        "\n",
        "        # initial fill ----------------------------------------------------------------------\n",
        "        X_filled = X.copy()\n",
        "        if self.init_fill_method == \"mean\":\n",
        "            col_means = xp.nanmean(X, axis=0)\n",
        "            inds = nan_mask\n",
        "            X_filled[inds] = col_means[xp.newaxis, :][inds]\n",
        "        else:  # zero\n",
        "            X_filled[nan_mask] = 0.0\n",
        "\n",
        "        # main iteration --------------------------------------------------------------------\n",
        "        prev_norm = xp.linalg.norm(X_filled)\n",
        "        for it in range(1, self.max_iters + 1):\n",
        "            # truncated SVD: cpu → scipy.sparse.linalg.svds; gpu → full svd of cuPy\n",
        "            if self.use_gpu:\n",
        "                U, S, Vt = xp.linalg.svd(X_filled, full_matrices=False)\n",
        "                U, S, Vt = U[:, :max_rank], S[:max_rank], Vt[:max_rank, :]\n",
        "            else:\n",
        "                # work with float64 for SciPy stability\n",
        "                U, S, Vt = _svds(_sp.csr_matrix(X_filled), k=max_rank, which=\"LM\")\n",
        "                # SciPy returns in ascending order\n",
        "                U, S, Vt = U[:, ::-1], S[::-1], Vt[::-1, :]\n",
        "\n",
        "            # soft‑threshold singular values ------------------------------------------------\n",
        "            S_shrink = _soft_threshold(S, self.lam)\n",
        "            rank_k = int((S_shrink > 0).sum())\n",
        "            if rank_k == 0:\n",
        "                warnings.warn(\"All singular values shrunk to 0 – returning previous iterate.\")\n",
        "                break\n",
        "            U = U[:, :rank_k]\n",
        "            S_shrink = S_shrink[:rank_k]\n",
        "            Vt = Vt[:rank_k, :]\n",
        "\n",
        "            # reconstruct and impute --------------------------------------------------------\n",
        "            X_hat = (U * S_shrink) @ Vt   # U (m×r) * diag(S) * V^T (r×n)\n",
        "            X_filled[nan_mask] = X_hat[nan_mask]\n",
        "\n",
        "            # convergence check -------------------------------------------------------------\n",
        "            frob_norm = xp.linalg.norm(X_filled)\n",
        "            rel_change = xp.linalg.norm(X_filled - X_hat) / max(1.0, frob_norm)\n",
        "            if rel_change < self.tol:\n",
        "                break\n",
        "            prev_norm = frob_norm\n",
        "\n",
        "        # store factors on CPU for compat ----------------------------------------------------\n",
        "        self.U_ = _cp.asnumpy(U) if self.use_gpu else U\n",
        "        self.S_ = _cp.asnumpy(S_shrink) if self.use_gpu else S_shrink\n",
        "        self.V_ = _cp.asnumpy(Vt.T) if self.use_gpu else Vt.T\n",
        "\n",
        "        if self.return_factors:\n",
        "            return self.U_, self.S_, self.V_\n",
        "        return _cp.asnumpy(X_filled) if self.use_gpu else X_filled\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    def transform(self, X_new: Array | Sparse) -> Array:\n",
        "        \"\"\"Impute a *new* matrix with the learnt factors (no retraining).\"\"\"\n",
        "        if self.U_ is None:\n",
        "            raise RuntimeError(\"call fit_transform first\")\n",
        "        X_new = _to_backend(X_new, self.use_gpu)\n",
        "        xp = _cp if self.use_gpu else _np\n",
        "        dense = X_new.copy()\n",
        "        nan_mask = xp.isnan(dense)\n",
        "        X_hat = (self.U_ * self.S_) @ self.V_.T\n",
        "        dense[nan_mask] = X_hat[nan_mask]\n",
        "        return _cp.asnumpy(dense) if self.use_gpu else dense\n",
        "# ============================================================================ #\n",
        "# CELL 7: Run Solvers and Compare Results - Renumbered\n",
        "# ============================================================================ #\n",
        "logger.info(\"+++ Cell 7: Running Solvers and Comparing Results +++\")\n",
        "\n",
        "all_results = {}\n",
        "# --- Initialize Trajectory Cache (Rank 0 only) ---\n",
        "TRAJECTORY_CACHE = [] if RANK_MPI == 0 else None\n",
        "\n",
        "# --- Update solver_args with new variable names ---\n",
        "solver_args = {\n",
        "    \"R_train_coo\": R_train_coo, \"global_mean\": global_mean_rating,\n",
        "    \"probe_users_mapped\": user_ids_val_final, \"probe_movies_mapped\": movie_ids_val_final,\n",
        "    \"probe_ratings_true\": ratings_val_true, \"N_users_active\": N_users_active,\n",
        "    \"M_movies_active\": M_movies_active, \"rank_local\": RANK, \"lam_sq\": LAM_SQ,\n",
        "    \"lam_bias\": LAM_BIAS, \"rng\": GLOBAL_RNG, \"init_scale\": INIT_SCALE_NON_CONVEX,\n",
        "}\n",
        "\n",
        "# --- Run Non-Convex Solvers ---\n",
        "if DATA_AVAILABLE and R_train_coo.nnz > 0 and N_users_active > 0 and M_movies_active > 0:\n",
        "    # Euclidean GD (NEW)\n",
        "    if RANK_MPI == 0: logger.info(\"\\n--- Running Non-Convex Solver (Euclidean GD with Biases) ---\")\n",
        "    try: all_results['Non-Convex (EucGD+Bias)'] = run_euclidean_gd(**solver_args, n_iters=N_ITERS_ALL, lr=1e-7) # Added call, specify LR\n",
        "    except Exception as e: logger.error(f\"EucGD Failed: {e}\", exc_info=True); all_results['Non-Convex (EucGD+Bias)'] = {}\n",
        "    # SVRG\n",
        "    if RANK_MPI == 0: logger.info(\"\\n--- Running Non-Convex Solver (SVRG Adaptation with Biases) ---\")\n",
        "    try: all_results['Non-Convex (SVRG+Bias)'] = run_non_convex_svrg_with_biases(**solver_args, n_epochs=N_ITERS_ALL, inner_lr=INIT_LR_SVRG, batch_size=RSVRG_BATCH_SIZE, max_grad_norm=GRAD_CLIP_THRESHOLD)\n",
        "    except Exception as e: logger.error(f\"SVRG Failed: {e}\", exc_info=True); all_results['Non-Convex (SVRG+Bias)'] = {}\n",
        "    # ALS\n",
        "    if RANK_MPI == 0: logger.info(\"\\n--- Running Non-Convex Solver (ALS with Biases) ---\")\n",
        "    try: all_results['Non-Convex (ALS+Bias)'] = run_als_with_biases(**solver_args, n_iters=N_ITERS_ALL, tol=ALS_TOL)\n",
        "    except Exception as e: logger.error(f\"ALS Failed: {e}\", exc_info=True); all_results['Non-Convex (ALS+Bias)'] = {}\n",
        "    # RGD\n",
        "    if RANK_MPI == 0: logger.info(\"\\n--- Running Non-Convex Solver (RGD with Biases) ---\")\n",
        "    try: all_results['Non-Convex (RGD+Bias)'] = run_rgd_with_biases(**solver_args, n_iters=N_ITERS_ALL, lr_init=INIT_LR_RIEMANN, ls_beta=LS_BETA, ls_sigma=LS_SIGMA)\n",
        "    except Exception as e: logger.error(f\"RGD Failed: {e}\", exc_info=True); all_results['Non-Convex (RGD+Bias)'] = {}\n",
        "    # RAGD\n",
        "    if RANK_MPI == 0: logger.info(\"\\n--- Running Non-Convex Solver (RAGD with Biases) ---\")\n",
        "    try: all_results['Non-Convex (RAGD+Bias)'] = run_ragd_with_biases(**solver_args, n_iters=N_ITERS_ALL, lr_init=INIT_LR_RIEMANN, ls_beta=LS_BETA, ls_sigma=LS_SIGMA, gamma=RAGD_GAMMA, mu=RAGD_MU, beta_ragd=RAGD_BETA)\n",
        "    except Exception as e: logger.error(f\"RAGD Failed: {e}\", exc_info=True); all_results['Non-Convex (RAGD+Bias)'] = {}\n",
        "    # Catalyst + Selected Inner Solver\n",
        "    if RANK_MPI == 0: logger.info(f\"\\n--- Running Non-Convex Solver (Catalyst-{INNER_SOLVER.upper()} with Biases) ---\")\n",
        "    try: all_results[f'Non-Convex (Catalyst+{INNER_SOLVER.upper()})'] = run_catalyst_stochastic(**solver_args, n_iters=N_ITERS_ALL, lr_init=INIT_LR_RIEMANN, ls_beta=LS_BETA, ls_sigma=LS_SIGMA, kappa_0=KAPPA_0, kappa_cvx=KAPPA_CVX, inner_T_epochs=CATALYST_INNER_T_EPOCHS, inner_S_epochs_base=CATALYST_INNER_S_EPOCHS_BASE, max_kappa_doublings=MAX_KAPPA_DOUBLINGS, inner_solver_type=INNER_SOLVER, inner_solver_lr=RSVRG_LR, inner_solver_bs=RSVRG_BATCH_SIZE)\n",
        "    except Exception as e: logger.error(f\"Catalyst-{INNER_SOLVER.upper()} Failed: {e}\", exc_info=True); all_results[f'Non-Convex (Catalyst+{INNER_SOLVER.upper()})'] = {}\n",
        "    # DANE\n",
        "    if RANK_MPI == 0: logger.info(\"\\n--- Running Non-Convex Solver (DANE with Biases) ---\")\n",
        "    try: all_results['Non-Convex (DANE+Bias)'] = run_dane_with_biases(**solver_args, n_iters=N_ITERS_ALL, lr_init=INIT_LR_RIEMANN, ls_beta=LS_BETA, ls_sigma=LS_SIGMA, kappa=DANE_KAPPA)\n",
        "    except Exception as e: logger.error(f\"DANE Failed: {e}\", exc_info=True); all_results['Non-Convex (DANE+Bias)'] = {}\n",
        "else:\n",
        "    if RANK_MPI == 0: logger.warning(\"Skipping Non-Convex Solvers due to missing data or zero dimensions.\")\n",
        "\n",
        "# --- Run Convex Solver (Efficient Soft-Impute) ---\n",
        "if DATA_AVAILABLE and R_train_coo_orig.nnz > 0 and N_users_active > 0 and M_movies_active > 0:\n",
        "    if RANK_MPI == 0: logger.info(\"\\n--- Running Convex Solver (Efficient Soft-Impute) ---\")\n",
        "    try:\n",
        "        results_convex = run_soft_impute_efficient(\n",
        "            R_train_coo_orig=R_train_coo_orig, # Use original ratings matrix\n",
        "            probe_users_mapped=user_ids_val_final,\n",
        "            probe_movies_mapped=movie_ids_val_final,\n",
        "            probe_ratings_true=ratings_val_true, # Use validation ratings\n",
        "            N_users_active=N_users_active,\n",
        "            M_movies_active=M_movies_active,\n",
        "            n_iters=N_ITERS_ALL, # Use N_ITERS_ALL for consistency\n",
        "            lambda_reg=LAM, # Use LAM directly\n",
        "            k_rank = CONVEX_RANK_K,\n",
        "            tol=SOFT_IMPUTE_TOL,\n",
        "            rng=GLOBAL_RNG\n",
        "        )\n",
        "        all_results['Convex (SoftImpute Eff.)'] = results_convex\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to run Efficient Soft-Impute Solver: {e}\", exc_info=True)\n",
        "        all_results['Convex (SoftImpute Eff.)'] = {'loss': [], 'rmse': [], 'time': [], 'rank': []}\n",
        "else:\n",
        "    if RANK_MPI == 0: logger.warning(\"Skipping Convex Solver due to missing data or zero dimensions.\")\n",
        "    all_results['Convex (SoftImpute Eff.)'] = {'loss': [], 'rmse': [], 'time': [], 'rank': []}\n",
        "\n",
        "\n",
        "# --- Plotting Comparison ---\n",
        "if RANK_MPI == 0:\n",
        "    logger.info(\"\\n--- Generating Comparison Plots ---\")\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(12, 11), sharex='col')\n",
        "    fig.suptitle(\n",
        "        f'MovieLens 1M ({RATING_LIMIT/1e6 if RATING_LIMIT else \"Full\"} M ratings subset), '\n",
        "        f'Rank={RANK}, Outer iters={N_ITERS_ALL})',\n",
        "        fontsize=14,\n",
        "    )\n",
        "\n",
        "    # ---------- style dictionary (matches earlier section) ------------------\n",
        "    styles = {\n",
        "        'Non-Convex (SVRG+Bias)': dict(label=r'SVRG+Bias', style=('-',  'p'), alpha=.90, color='tab:purple'),\n",
        "        'Non-Convex (ALS+Bias)': dict(label=r'ALS+Bias', style=('-',  'v'), alpha=.90, color='tab:brown'),\n",
        "        'Non-Convex (RGD+Bias)': dict(label=r'RGD+Bias', style=('--', 'o'), alpha=.80, color='tab:blue'),\n",
        "        'Non-Convex (RAGD+Bias)': dict(label=r'RAGD+Bias', style=('-.', 'D'), alpha=.80, color='tab:orange'),\n",
        "        f'Non-Convex (Catalyst+{INNER_SOLVER.upper()})': dict(label=f'Catalyst+{INNER_SOLVER.upper()}', style=('-',  's'), alpha=.90, color='tab:red'),\n",
        "        'Non-Convex (DANE+Bias)': dict(label=r'DANE+Bias', style=('-',  'x'), alpha=.80, color='tab:cyan'),\n",
        "        'Non-Convex (EucGD+Bias)': dict(label=r'EucGD+Bias', style=(':',  '^'), alpha=.70, color='tab:green'),\n",
        "        'Convex (SoftImpute Eff.)': dict(label=r'SoftImpute (Eff)', style=('-',  '*'), alpha=.90, color='tab:pink'),\n",
        "    }\n",
        "\n",
        "    # ---------- helper for plotting one method ------------------------------\n",
        "    def _plot(ax_iter, ax_time, data, meta):\n",
        "        ls, mk = meta['style']\n",
        "        kw = dict(linestyle=ls, marker=mk, markersize=3, alpha=meta['alpha'], color=meta.get('color', None))\n",
        "        n_loss = len(data.get('loss', [])); n_grad = len(data.get('grad_norm', [])); n_rmse = len(data.get('rmse', [])); n_time = len(data.get('time', []))\n",
        "        n = min(n_loss if n_loss > 0 else float('inf'), n_grad if n_grad > 0 else float('inf'), n_rmse if n_rmse > 0 else float('inf'), n_time if n_time > 0 else float('inf'))\n",
        "        if n == float('inf') or n < 2: logger.warning(f\"  • insufficient points for {meta['label']}\"); return\n",
        "\n",
        "        it = np.arange(n)\n",
        "        loss_vals = np.array(data.get('loss', [np.nan]*n)[:n]); grad_vals = np.array(data.get('grad_norm', [np.nan]*n)[:n])\n",
        "        rmse_vals = np.array(data.get('rmse', [np.nan]*n)[:n]); time_vals = np.array(data.get('time', [np.nan]*n)[:n])\n",
        "\n",
        "        # Determine primary metric for grad plot (grad_norm, or gU_norm for SVRG)\n",
        "        grad_metric = grad_vals\n",
        "        if not np.any(np.isfinite(grad_metric)) and 'gU_norm' in data:\n",
        "             grad_metric = np.array(data.get('gU_norm', [np.nan]*n)[:n])\n",
        "\n",
        "        loss_ok = np.isfinite(loss_vals); grad_ok = np.isfinite(grad_metric); rmse_ok = np.isfinite(rmse_vals); time_ok = np.isfinite(time_vals)\n",
        "\n",
        "        # iteration domain\n",
        "        if np.any(loss_ok): ax_iter[0].semilogy(it[loss_ok], loss_vals[loss_ok], label=meta['label'], **kw)\n",
        "        if np.any(grad_ok): ax_iter[1].semilogy(it[grad_ok], grad_metric[grad_ok], **kw)\n",
        "        if np.any(rmse_ok): ax_iter[2].plot(it[rmse_ok], rmse_vals[rmse_ok], **kw)\n",
        "\n",
        "        # wall-clock domain\n",
        "        if np.any(loss_ok & time_ok): ax_time[0].semilogy(time_vals[loss_ok & time_ok], loss_vals[loss_ok & time_ok], **kw)\n",
        "        if np.any(grad_ok & time_ok): ax_time[1].semilogy(time_vals[grad_ok & time_ok], grad_metric[grad_ok & time_ok], **kw)\n",
        "        if np.any(rmse_ok & time_ok): ax_time[2].plot(time_vals[rmse_ok & time_ok], rmse_vals[rmse_ok & time_ok], **kw)\n",
        "\n",
        "    # ---------- draw every available method ---------------------------------\n",
        "    for m, d in all_results.items():\n",
        "        if m in styles and d: # Check if history dict is not empty\n",
        "            _plot(axes[:, 0], axes[:, 1], d, styles[m])\n",
        "        else:\n",
        "            logger.warning(f\"  • no style or no results for '{m}', skipped.\")\n",
        "\n",
        "    # labels / titles\n",
        "    axes[0,0].set_ylabel('Objective'); axes[0,0].set_title('Loss vs Iterations')\n",
        "    axes[1,0].set_ylabel(r'$\\|\\nabla\\|$'); axes[1,0].set_title('Grad-norm vs Iterations')\n",
        "    axes[2,0].set_ylabel('Validation RMSE'); axes[2,0].set_xlabel('Iteration k'); axes[2,0].set_title('RMSE vs Iterations')\n",
        "    axes[0,1].set_xscale('log'); axes[0,1].set_ylabel('Objective'); axes[0,1].set_title('Loss vs Wall-time')\n",
        "    axes[1,1].set_xscale('log'); axes[1,1].set_ylabel(r'$\\|\\nabla\\|$'); axes[1,1].set_title('Grad-norm vs Wall-time')\n",
        "    axes[2,1].set_xscale('log'); axes[2,1].set_ylabel('Validation RMSE'); axes[2,1].set_xlabel('Seconds'); axes[2,1].set_title('RMSE vs Wall-time')\n",
        "\n",
        "    for ax in axes.flatten():\n",
        "        ax.grid(True, which='both', linestyle=':', linewidth=.5)\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        if handles: ax.legend() # Only add legend if there are labeled artists\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    # ---------- optional PCA trajectory plot --------------------------------\n",
        "    if PCA_AVAILABLE and TRAJECTORY_CACHE is not None and len(TRAJECTORY_CACHE) >= 3:\n",
        "        logger.info(\"\\n+++ Generating PCA Trajectory Plot +++\")\n",
        "        try:\n",
        "            traj_dim = TRAJECTORY_CACHE[0].size\n",
        "            valid_traj = [t for t in TRAJECTORY_CACHE if isinstance(t, np.ndarray) and t.size == traj_dim]\n",
        "            if len(valid_traj) >= 3:\n",
        "                 pcs = PCA(n_components=2).fit_transform(np.vstack(valid_traj))\n",
        "                 plt.figure(figsize=(4.5,4)); plt.plot(pcs[:,0], pcs[:,1], '-o', markersize=3)\n",
        "                 plt.title('Optimisation Trajectory (PCA)'); plt.xlabel('PC1'); plt.ylabel('PC2')\n",
        "                 plt.tight_layout(); plt.show()\n",
        "            else: logger.warning(\"Not enough valid trajectory points for PCA plot.\")\n",
        "        except Exception as e_pca: logger.error(f\"PCA Trajectory plot failed: {e_pca}\")\n",
        "\n",
        "\n",
        "# --- Final Summary Table ---\n",
        "if RANK_MPI == 0:\n",
        "    logger.info(\"\\n--- Final Comparison Summary ---\")\n",
        "    print(f\"{'Method':<30} | {'Final RMSE':<15} | {'Final Loss':<15} | {'Final Rank/GradNorm':<18} | {'Time (s)':<15}\")\n",
        "    print(f\"{'-'*30}-|-{'-'*15}-|-{'-'*15}-|-{'-'*18}-|-{'-'*15}\")\n",
        "    def get_last_finite(history, key):\n",
        "        if not isinstance(history, dict): return np.nan\n",
        "        data = history.get(key)\n",
        "        if isinstance(data, (list, np.ndarray)) and len(data) > 0:\n",
        "            arr = np.array(data); finite_vals = arr[np.isfinite(arr)]\n",
        "            return finite_vals[-1] if finite_vals.size > 0 else np.nan\n",
        "        return np.nan\n",
        "    for label, history in all_results.items():\n",
        "        if not history: print(f\"{label:<30} | {'FAILED':<15} | {'FAILED':<15} | {'N/A':<18} | {'N/A':<15}\"); continue\n",
        "        final_rmse = get_last_finite(history, 'rmse')\n",
        "        final_loss = get_last_finite(history, 'loss')\n",
        "        final_time = get_last_finite(history, 'time')\n",
        "        final_rank = get_last_finite(history, 'rank') if 'rank' in history else RANK\n",
        "        final_grad_norm = get_last_finite(history, 'grad_norm') if 'grad_norm' in history else np.nan\n",
        "        final_gU_norm = get_last_finite(history, 'gU_norm') if 'gU_norm' in history else np.nan\n",
        "        rmse_str = f\"{final_rmse:.6f}\" if np.isfinite(final_rmse) else 'NaN'\n",
        "        loss_str = f\"{final_loss:.6e}\" if np.isfinite(final_loss) and 'ALS' not in label and 'SoftImpute' not in label else 'N/A'\n",
        "        rank_or_grad_str = 'N/A'\n",
        "        if 'SoftImpute' in label: rank_or_grad_str = f\"Rank={int(final_rank)}\" if np.isfinite(final_rank) else 'N/A'\n",
        "        elif 'grad_norm' in history and np.isfinite(final_grad_norm): rank_or_grad_str = f\"||G||={final_grad_norm:.2e}\"\n",
        "        elif 'gU_norm' in history and np.isfinite(final_gU_norm): rank_or_grad_str = f\"||gU||={final_gU_norm:.2e}\"\n",
        "        else: rank_or_grad_str = f\"Rank={RANK}\"\n",
        "        time_str = f\"{final_time:.4f}\" if np.isfinite(final_time) else 'N/A'\n",
        "        print(f\"{label:<30} | {rmse_str:<15} | {loss_str:<15} | {rank_or_grad_str:<18} | {time_str:<15}\")\n",
        "    print(\"\\nComparison Complete.\")\n",
        "\n",
        "# --- ADDED Block 6-a: Run OT Demo (Rank 0 only) ---\n",
        "# --- ADDED Block 6-a: Run OT Demo (Rank 0 only) ---\n",
        "if RANK_MPI == 0 and OT_AVAILABLE:\n",
        "    logger.info(\"\\n+++ Running OT Barycentre Demo +++\")\n",
        "    try:\n",
        "        ot_demo_results = run_barycentre_demo()\n",
        "        # Optionally plot or process ot_demo_results\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.plot(ot_demo_results['grid'], ot_demo_results['sources'], '--', label='Sources')\n",
        "        plt.plot(ot_demo_results['grid'], ot_demo_results['barycenter'], 'r-', label='Barycenter')\n",
        "        plt.title('Wasserstein Barycenter Demo')\n",
        "        plt.legend(); plt.tight_layout(); plt.show()\n",
        "    except Exception as e_ot:\n",
        "        logger.error(f\"OT Barycentre Demo failed: {e_ot}\")\n",
        "\n",
        "# === ADDED Block 6: PCA Trajectory Plot (Rank 0 only) ===\n",
        "if RANK_MPI == 0 and PCA_AVAILABLE and len(TRAJECTORY_CACHE) >= 3:\n",
        "    logger.info(\"\\n+++ Generating PCA Trajectory Plot +++\")\n",
        "    try:\n",
        "        # Ensure all trajectories have the same dimension (flattened U)\n",
        "        traj_dim = TRAJECTORY_CACHE[0].size\n",
        "        valid_traj = [t for t in TRAJECTORY_CACHE if t.size == traj_dim]\n",
        "        if len(valid_traj) >= 3:\n",
        "             pcs = PCA(n_components=2).fit_transform(np.vstack(valid_traj))\n",
        "             plt.figure(figsize=(4.5,4)); plt.plot(pcs[:,0], pcs[:,1], '-o', markersize=3)\n",
        "             plt.title('Optimisation Trajectory (PCA)'); plt.xlabel('PC1'); plt.ylabel('PC2')\n",
        "             plt.tight_layout(); plt.show()\n",
        "        else:\n",
        "             logger.warning(\"Not enough valid trajectory points for PCA plot.\")\n",
        "    except Exception as e_pca:\n",
        "         logger.error(f\"PCA Trajectory plot failed: {e_pca}\")\n",
        "\n",
        "# === ADDED Block 7: Dump TeX skeleton to Drive (Rank 0 only) ===\n",
        "if RANK_MPI == 0:\n",
        "    TEX_PATH = Path(DATA_DIR_STR) / \"proofs.tex\" # Use Path object\n",
        "    if TEX_PATH.parent.is_dir():\n",
        "        logger.info(f\"\\n+++ Checking/Writing TeX Proof Skeleton to: {TEX_PATH} +++\")\n",
        "        if not TEX_PATH.exists():\n",
        "            try:\n",
        "                with open(TEX_PATH, \"w\") as f: f.write(r\"\"\"...\"\"\") # TeX content omitted for brevity\n",
        "                logger.info(f\"  Wrote TeX scaffold to {TEX_PATH}\")\n",
        "            except IOError as e: logger.error(f\"  Error writing TeX file: {e}\")\n",
        "        else: logger.info(f\"  TeX scaffold already exists at {TEX_PATH}, not overwritten.\")\n",
        "    else: logger.warning(f\"  Parent directory for TeX file not found: {TEX_PATH.parent}\")\n",
        "\n",
        "\n",
        "# --- Mount Drive (if not already mounted) ---\n",
        "if RANK_MPI == 0 and not Path(\"/content/drive\").is_mount():\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to mount Google Drive at the end: {e}\")\n",
        "\n",
        "# ============================================================================ #\n",
        "# CELL 8: Plots & Dashboards (from long.txt) - Renumbered\n",
        "# ============================================================================ #\n",
        "if RANK_MPI == 0:\n",
        "    logger.info(\"\\n+++ Cell 8: Plots & Dashboards +++\")\n",
        "\n",
        "    # ---------- helper ---------------------------------------------------- #\n",
        "def _plot_metric(metric_key: str,\n",
        "                 ylabel: str,\n",
        "                 x_key: str = \"time\",\n",
        "                 title: str | None = None,\n",
        "                 logy: bool = False,\n",
        "                 logx: bool = True,          # Default: log time axis\n",
        "                 figsize=(8, 5)) -> None:\n",
        "    plt.figure(figsize=figsize)\n",
        "    has_data_to_plot = False\n",
        "\n",
        "    # style dictionary ----------------------------------------------------\n",
        "    styles = {\n",
        "        'Non-Convex (SVRG+Bias)':      dict(label='SVRG+Bias',      style=('-',  'p'), alpha=.90, color='tab:purple'),\n",
        "        'Non-Convex (ALS+Bias)':       dict(label='ALS+Bias',       style=('-',  'v'), alpha=.90, color='tab:brown'),\n",
        "        'Non-Convex (RGD+Bias)':       dict(label='RGD+Bias',       style=('--', 'o'), alpha=.80, color='tab:blue'),\n",
        "        'Non-Convex (RAGD+Bias)':      dict(label='RAGD+Bias',      style=('-.', 'D'), alpha=.80, color='tab:orange'),\n",
        "        f'Non-Convex (Catalyst+{INNER_SOLVER.upper()})':\n",
        "                                       dict(label=f'Catalyst+{INNER_SOLVER.upper()}',\n",
        "                                            style=('-',  's'), alpha=.90, color='tab:red'),\n",
        "        'Non-Convex (DANE+Bias)':      dict(label='DANE+Bias',      style=('-',  'x'), alpha=.80, color='tab:cyan'),\n",
        "        'Non-Convex (EucGD+Bias)':     dict(label='EucGD+Bias',     style=(':',  '^'), alpha=.70, color='tab:green'),\n",
        "        'Convex (SoftImpute Eff.)':    dict(label='SoftImpute (Eff)',style=('-', '*'), alpha=.90, color='tab:pink'),\n",
        "    }\n",
        "\n",
        "    # loop over solver results -------------------------------------------\n",
        "    for name, res in all_results.items():\n",
        "        y = res.get(metric_key, [])\n",
        "        x = res.get(x_key, list(range(len(y)))) if x_key else list(range(len(y)))\n",
        "\n",
        "        if len(y) == 0:\n",
        "            continue\n",
        "\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        y = np.asarray(y, dtype=float)\n",
        "        valid = np.isfinite(x) & np.isfinite(y)\n",
        "        x_plot, y_plot = x[valid], y[valid]\n",
        "\n",
        "        if x_plot.size == 0:\n",
        "            logger.warning(f\"No finite data to plot for {name} – {metric_key}\")\n",
        "            continue\n",
        "\n",
        "        style = styles.get(name, {})\n",
        "        plt.plot(\n",
        "            x_plot, y_plot,\n",
        "            linestyle=style.get('style', ('-', 'o'))[0],\n",
        "            marker=style.get('style', ('-', 'o'))[1],\n",
        "            markersize=3,\n",
        "            alpha=style.get('alpha', 0.8),\n",
        "            color=style.get('color'),\n",
        "            label=style.get('label', name)\n",
        "        )\n",
        "        has_data_to_plot = True\n",
        "\n",
        "    # axes / formatting ---------------------------------------------------\n",
        "    plt.xlabel(\"wall-clock (s)\" if x_key == \"time\" else \"iteration\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if logx:\n",
        "        plt.xscale(\"log\")\n",
        "    if logy:\n",
        "        plt.yscale(\"log\")\n",
        "    plt.title(title or f\"{ylabel} vs {'time' if x_key == 'time' else 'iteration'}\")\n",
        "    if has_data_to_plot:\n",
        "        plt.legend()\n",
        "    plt.grid(alpha=.3, which='both', linestyle=':')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ---------- Summary Table ----------\n",
        "    logger.info(\"\\n--- Final Comparison Summary (Pandas) ---\")\n",
        "    summary = []\n",
        "    for name, res in all_results.items():\n",
        "        if isinstance(res, dict) and res.get(\"rmse\"):\n",
        "            best_rmse = min([v for v in res[\"rmse\"] if np.isfinite(v)], default=np.nan)\n",
        "            final_rmse = get_last_finite(res, \"rmse\")\n",
        "            final_time = get_last_finite(res, \"time\")\n",
        "            summary.append({\"solver\": name, \"best RMSE\": best_rmse, \"final RMSE\": final_rmse, \"train time (s)\": final_time})\n",
        "    if summary:\n",
        "        summary_df = pd.DataFrame(summary).sort_values(\"best RMSE\")\n",
        "        display(summary_df) # Use display for Colab/Jupyter\n",
        "    else: logger.warning(\"No valid results to display in summary table.\")\n",
        "\n",
        "    # ---------- Save Figures ----------\n",
        "    FIG_DIR = Path(DATA_DIR_STR) / \"figs\"\n",
        "    try:\n",
        "        FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
        "        logger.info(f\"Saving figures to {FIG_DIR}...\")\n",
        "        for n, fig_num in enumerate(plt.get_fignums(), 1):\n",
        "            plt.figure(fig_num)\n",
        "            fig_path = FIG_DIR / f\"solver_plot_{n:02d}.png\"\n",
        "            plt.savefig(fig_path, dpi=180, bbox_inches='tight')\n",
        "            print(\"  saved →\", fig_path)\n",
        "    except Exception as e_fig: logger.error(f\"Could not save figures: {e_fig}\")\n",
        "\n",
        "    print(\"\\n✅ All comparisons finished.\")\n",
        "\n",
        "# --- ADDED Block 6-a: Run OT Demo (Rank 0 only) ---\n",
        "if RANK_MPI == 0 and OT_AVAILABLE:\n",
        "    logger.info(\"\\n+++ Running OT Barycentre Demo +++\")\n",
        "    try:\n",
        "        if 'run_barycentre_demo' in globals():\n",
        "            ot_demo_results = run_barycentre_demo()\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            plt.plot(ot_demo_results['grid'], ot_demo_results['sources'], '--', label='Sources')\n",
        "            plt.plot(ot_demo_results['grid'], ot_demo_results['barycenter'], 'r-', label='Barycenter')\n",
        "            plt.title('Wasserstein Barycenter Demo'); plt.legend(); plt.tight_layout(); plt.show()\n",
        "        else: logger.warning(\"run_barycentre_demo function not defined. Skipping OT demo.\")\n",
        "    except Exception as e_ot: logger.error(f\"OT Barycentre Demo failed: {e_ot}\")\n",
        "\n",
        "# === ADDED Block 6: PCA Trajectory Plot (Rank 0 only) ===\n",
        "if RANK_MPI == 0 and PCA_AVAILABLE and TRAJECTORY_CACHE is not None and len(TRAJECTORY_CACHE) >= 3:\n",
        "    logger.info(\"\\n+++ Generating PCA Trajectory Plot +++\")\n",
        "    try:\n",
        "        traj_dim = TRAJECTORY_CACHE[0].size\n",
        "        valid_traj = [t for t in TRAJECTORY_CACHE if isinstance(t, np.ndarray) and t.size == traj_dim]\n",
        "        if len(valid_traj) >= 3:\n",
        "             pcs = PCA(n_components=2).fit_transform(np.vstack(valid_traj))\n",
        "             plt.figure(figsize=(4.5,4)); plt.plot(pcs[:,0], pcs[:,1], '-o', markersize=3)\n",
        "             plt.title('Optimisation Trajectory (PCA)'); plt.xlabel('PC1'); plt.ylabel('PC2')\n",
        "             plt.tight_layout(); plt.show()\n",
        "        else: logger.warning(\"Not enough valid trajectory points for PCA plot.\")\n",
        "    except Exception as e_pca: logger.error(f\"PCA Trajectory plot failed: {e_pca}\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.11/dist-packages (4.0.3)\n",
            "Requirement already satisfied: POT in /usr/local/lib/python3.11/dist-packages (0.9.5)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.11/dist-packages (from POT) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.11/dist-packages (from POT) (1.15.2)\n",
            "Sun May  4 22:03:29 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement softimpute (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for softimpute\u001b[0m\u001b[31m\n",
            "\u001b[0mMounted at /content/drive\n",
            "+++ MPI Detected: Running with 1 processes. +++\n",
            "+++ Mounting Google Drive +++\n",
            "Mounted at /content/drive\n",
            "Drive mounted.\n",
            "Calculating importance sampling weights...\n",
            "Importance sampling enabled (weights based on 800167 ratings).\n",
            "RSVRG Epoch Length set to 61 batches.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  EucGD Iter 01 | Loss: 4.991e+05 | GradNorm: 3.091e+01 | RMSE: 1.1177 | Time: 0.53s\n",
            "  EucGD Iter 06 | Loss: 4.990e+05 | GradNorm: 3.090e+01 | RMSE: 1.1176 | Time: 0.52s\n",
            "  EucGD Iter 11 | Loss: 4.989e+05 | GradNorm: 3.090e+01 | RMSE: 1.1175 | Time: 0.52s\n",
            "  EucGD Iter 16 | Loss: 4.988e+05 | GradNorm: 3.090e+01 | RMSE: 1.1173 | Time: 0.52s\n",
            "  EucGD Iter 20 | Loss: 4.988e+05 | GradNorm: 3.090e+01 | RMSE: 1.1173 | Time: 0.52s\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "3NHgqPNgUQqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79ae86c4-1da4-41ac-894d-6b91f80824ac"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}